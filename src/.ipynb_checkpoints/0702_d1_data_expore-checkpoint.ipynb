{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARR SEQ 6/29 - 7/2 RUN REPORT #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyfaidx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89d73a43ff90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LOADS BIOLOGICAL MOTIFS AND SCANS ALL SUBREGIONS FOR OCCURENCES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyfaidx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFasta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msequences_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFasta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cluster/bh0085/genomes/GRCh38.primary_assembly.genome.fa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchrseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_fa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chr22\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyfaidx'"
     ]
    }
   ],
   "source": [
    "#LOADS BIOLOGICAL MOTIFS AND SCANS ALL SUBREGIONS FOR OCCURENCES\n",
    "from pyfaidx import Fasta\n",
    "sequences_fa = Fasta('/cluster/bh0085/genomes/GRCh38.primary_assembly.genome.fa')\n",
    "chrseq = str(sequences_fa[\"chr22\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import z2_save_jaspar\n",
    "jaspar= z2_save_jaspar.load_jaspar()\n",
    "import z1_save_oligos\n",
    "oligos,oligos_by_exp = z1_save_oligos.load_oligos()\n",
    "\n",
    "MODERATE_EXPR_MU = 15\n",
    "HIGH_EXPR_MU = 50\n",
    "EXP_MODERATE_EXPR_MU = 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFY ENHANCERS IN POOLED DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enriched_oligos(oligos_df, MU_THRESHOLD):\n",
    "    #categorize oligo groups by mean expression\n",
    "\n",
    "    oligos_lib = z1_save_oligos.oligos_lib\n",
    "    unique_starts = pd.DataFrame(pd.Series(oligos_lib.starts.unique()).rename(\"starts\"))\n",
    "    start_indexes = unique_starts.reset_index().set_index(\"starts\")\n",
    "\n",
    "    starts_by_mean_expression = oligos_df.groupby(\"start_index\").mu.mean().sort_values(ascending=False)\n",
    "    enriched_oligo_starts = starts_by_mean_expression.loc[(starts_by_mean_expression>=MU_THRESHOLD)]\n",
    "    enriched_oligo_start_positions =unique_starts.loc[enriched_oligo_starts.index].starts\n",
    "    enriched_oligo_start_neighborhoods = pd.concat([enriched_oligo_start_positions + i * 30 for i in range(-2,3)])\n",
    "\n",
    "    oligos_by_start = oligos.reset_index().set_index(\"starts\").sort_index()\n",
    "    active_oligos = oligos_by_start.loc[oligos_by_start.groupby(level=0).size() == 5]\n",
    "    enriched_oligos = active_oligos.loc[active_oligos.index.isin(enriched_oligo_start_positions)]\n",
    "    enriched_motif_candidates  = enriched_oligos.loc[enriched_oligos.groupby(level=0).apply(lambda x: x.mu.loc[x.mu > (x.mu.mean() - x.mu.std())].count() ==4)].groupby(level=0).apply(lambda x:x.sort_values(\"mu\",ascending=True).iloc[0])\n",
    "    enriched_oligos = enriched_oligos.reset_index().set_index(\"oligo\")\n",
    "    \n",
    "    return enriched_oligos\n",
    "\n",
    "def get_enhancers(enriched_oligos):\n",
    "    oligo_candidates = enriched_oligos\n",
    "    candidate_min_mutant_nums = oligo_candidates.reset_index().groupby(\"starts\").apply(lambda x: x.sort_values(\"mu\").iloc[0].mutant_num)\n",
    "\n",
    "    enhancer_mutant_candidates=  oligo_candidates.loc[\n",
    "        (oligo_candidates.starts.isin(candidate_min_mutant_nums.index[candidate_min_mutant_nums>0]))\n",
    "    ]\n",
    "    candidates_by_start = enhancer_mutant_candidates.reset_index().groupby(\"starts\").apply(lambda x: x.sort_values(\"mu\").iloc[0])\n",
    "    candidates_by_start[\"wt_sequence\"] = oligo_candidates.loc[oligo_candidates.mutant_num==0].set_index(\"starts\").Sequences\n",
    "    enhancer_mut_sequences_by_start = pd.concat([\n",
    "        candidates_by_start.apply(lambda x: x.wt_sequence[int((x.mutant_num-1) *30 ):int((x.mutant_num-1) *30+30)],axis=1).rename(\"seq\"),\n",
    "        candidates_by_start.wt_sequence,\n",
    "        candidates_by_start.oligo,\n",
    "        candidates_by_start.apply(lambda x: x.name,axis = 1).rename(\"start\"),\n",
    "        (candidates_by_start.apply(lambda x: x.name,axis = 1) + (candidates_by_start.mutant_num-1)*30).rename(\"mutant_start\")  \n",
    "    ],axis =1)\n",
    "    return enhancer_mut_sequences_by_start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYZE MOTIFS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_oligos =  get_enriched_oligos(oligos, MODERATE_EXPR_MU)\n",
    "enhancer_mut_sequences_by_start = get_enhancers(enriched_oligos)\n",
    "pooled_enhancers = enhancer_mut_sequences_by_start.rename_axis(\"starts\",axis=\"index\").reset_index()\n",
    "\n",
    "exp_enhancers = pd.DataFrame()\n",
    "for k,exp_oligos in oligos_by_exp.groupby(\"exp\"):\n",
    "    these_oligos = exp_oligos.reset_index(level=0)\n",
    "    exp_enriched_oligos = get_enriched_oligos(these_oligos, EXP_MODERATE_EXPR_MU)\n",
    "    exp_enhancer_mut_sequences_by_start = get_enhancers(exp_enriched_oligos)\n",
    "    exp_enhancer_mut_sequences_by_start[\"exp\"] = k\n",
    "    exp_enhancer_mut_sequences = exp_enhancer_mut_sequences_by_start.rename_axis(\"starts\", axis=\"index\").reset_index()\n",
    "    exp_enhancers = exp_enhancers.append(exp_enhancer_mut_sequences,ignore_index=True)\n",
    "    \n",
    "all_enhancer_candidates = pd.concat([pooled_enhancers, exp_enhancers], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_enhancer_candidates[\"60bp_sequence\"] = all_enhancer_candidates.mutant_start.apply(lambda x:chrseq[x-15+38699734:x-15+60+38699734])\n",
    "all_30bp_enhancer_sequences = all_enhancer_candidates.drop_duplicates(subset=[\"seq\"]).seq.rename_axis(\"id\",axis=\"index\")\n",
    "all_60bp_enhancer_sequences = all_enhancer_candidates.drop_duplicates(subset=[\"seq\"])[\"60bp_sequence\"].rename_axis(\"id\",axis=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_enhancer_candidates[[\"exp\",\"60bp_sequence\"]].rename({\"60bp_sequence\":\"seq\"},axis=\"columns\").\\\n",
    "    to_csv(\"../data/0711_fewerregions_with_exps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Seq, Alphabet\n",
    "seqseq  = all_60bp_enhancer_sequences.apply(lambda x: Seq.Seq(x,alphabet=Alphabet.IUPAC.IUPACUnambiguousDNA()))\n",
    "pwm_prod_grid = jaspar.pssm.apply(lambda  pssm: seqseq.apply(lambda x:  sorted(pssm.search(x),key=lambda x:-1*x[1])))\n",
    "pwm_scores_grid = pwm_prod_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][1]).unstack().T\n",
    "pwm_indices_grid = pwm_prod_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][0]).unstack().T\n",
    "max_motif_ids = pd.DataFrame(pwm_scores_grid.idxmax().rename(\"jaspar_id\"))\n",
    "\n",
    "pwm_scores_grid2 = pwm_scores_grid.copy()\n",
    "pwm_scores_grid2.columns = all_60bp_enhancer_sequences\n",
    "\n",
    "#ok_motif_hits = decent_motif_hits = pwm_scores_grid2.apply(lambda x: x / jaspar.threshold_patser).where(lambda x: x>1).reset_index().melt(id_vars =[\"jaspar_id\"]).dropna()\n",
    "ok_motif_hits = pwm_scores_grid2.apply(lambda x: x / jaspar.threshold_patser).where(lambda x: x>1).reset_index().melt(id_vars =[\"jaspar_id\"]).dropna().set_index(\"60bp_sequence\")[[\"jaspar_id\"]]\n",
    "decent_motif_hits = pwm_scores_grid2.apply(lambda x: x / jaspar.threshold_patser).where(lambda x: x>2).reset_index().melt(id_vars =[\"jaspar_id\"]).dropna().set_index(\"60bp_sequence\")[[\"jaspar_id\"]]\n",
    "great_motif_hits = pwm_scores_grid2.apply(lambda x: x / jaspar.threshold_patser).where(lambda x: x>4).reset_index().melt(id_vars =[\"jaspar_id\"]).dropna().set_index(\"60bp_sequence\")[[\"jaspar_id\"]]\n",
    "\n",
    "decent_motif_hits.index.name = \"seq\"\n",
    "great_motif_hits.index.name = \"seq\"\n",
    "ok_motif_hits.index.name = \"seq\"\n",
    "\n",
    "ok_motif_hits.to_csv(\"../data/0711_motif_fewerregions_okhits.csv\")\n",
    "decent_motif_hits.to_csv(\"../data/0711_motif_fewerregions_decenthits.csv\")\n",
    "great_motif_hits.to_csv(\"../data/0711_motif_fewerregions_greathits.csv\")\n",
    "max_motif_ids.set_index(all_60bp_enhancer_sequences.loc[max_motif_ids.index].rename(\"seq\")).reset_index().to_csv(\"../data/0711_motif_fewerregions_allbest.csv\")\n",
    "\n",
    "pd.DataFrame(all_60bp_enhancer_sequences.rename(\"seq\")).to_csv(\"../data/0711_fewerregions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutants(these_oligos, enhancers, search_sequences, pwm_scores_grid, pwm_indices_grid, jaspar):\n",
    "\n",
    "    wt_oligos = these_oligos.loc[these_oligos.mutant_num == 0]\n",
    "    #mut_sequences[\"max_expressed_wt_oligo\"]\n",
    "    wt_starts = wt_oligos.reset_index().set_index(\"starts\")\n",
    "\n",
    "    max_starts_by_sequence = enhancers.groupby(\"seq\")\\\n",
    "        .apply(lambda x: wt_starts.loc[x.start.values].mu.idxmin()) \n",
    "    max_oligos_by_sequence =max_starts_by_sequence.apply(lambda x: wt_starts.loc[x].oligo)\n",
    "\n",
    "    mseq_groups = enhancers.reset_index().groupby(\"seq\")\n",
    "    #this oligo has the most difference between wild-type and motif expression\n",
    "#     max_delta_mut_oligo =mseq_groups.apply(lambda x: \\\n",
    "#                                        (x.apply(lambda y:wt_starts.loc[y.start].mu,axis=1) \\\n",
    "#                                         - oligos.loc[x.oligo].mu).idxmax())\n",
    "\n",
    "\n",
    "    delta = enhancers.apply(lambda x: these_oligos.loc[x.name].mu,axis = 1) - enhancers.apply(lambda x:wt_starts.loc[x.start],axis = 1).mu\n",
    "    max_delta_mut_oligo = pd.concat([enhancers,delta.rename(\"wt_delta\")],axis = 1).groupby(\"seq\").apply(lambda x:x.wt_delta.idxmin())\n",
    "    max_delta_wt_oligo = max_delta_mut_oligo.apply(lambda x:wt_starts.loc[these_oligos.loc[x].starts].oligo)\n",
    "    mutants = enhancers.drop_duplicates(subset = [\"seq\"]).set_index(\"seq\")[[\"mutant_start\"]]\n",
    "\n",
    "    mutants[\"highest_overlapping_wt_oligo\"] = max_oligos_by_sequence\n",
    "    mutants[\"max_delta_wt_oligo\"] = max_delta_wt_oligo\n",
    "    mutants[\"max_delta_mut_oligo\"] = max_delta_mut_oligo\n",
    "    mutants[\"wt_mu\"] = mutants.max_delta_wt_oligo.apply(lambda x:these_oligos.mu.loc[x])\n",
    "    mutants[\"mut_mu\"] = mutants.max_delta_mut_oligo.apply(lambda x:these_oligos.mu.loc[x])\n",
    "    mutants[\"start_index\"] = mutants.max_delta_wt_oligo.apply(lambda x:these_oligos.start_index.loc[x])\n",
    "\n",
    "    mutants = mutants.reset_index()\n",
    "    mutants.index.name = \"index\"\n",
    "    #bad practice... should just save sequence indices in the pwm dataframe\n",
    "    seqids = search_sequences.reset_index().set_index(\"seq\")\n",
    "\n",
    "    mutants[\"best_jaspar_id\"] = mutants.apply(lambda x: pwm_scores_grid.T.loc[seqids.loc[x.seq].iloc[0]].idxmax(),axis=1)\n",
    "    mutants[\"best_jaspar_offset\"] = mutants.apply(lambda x: pwm_indices_grid.T.loc[seqids.loc[x.seq].iloc[0]][x.best_jaspar_id],axis = 1)\n",
    "    mutants[\"best_jaspar_score\"] = mutants.apply(lambda x: pwm_scores_grid.T.loc[seqids.loc[x.seq].iloc[0]].max(),axis=1)\n",
    "    mutants[\"jid\"] = mutants[\"best_jaspar_id\"]\n",
    "    mutants[\"best_jaspar_genome_start\"] =  mutants.mutant_start + mutants.best_jaspar_offset + (mutants.best_jaspar_offset <0)*30\n",
    "    mutants[\"best_jaspar_motif_start\"]= mutants[\"best_jaspar_genome_start\"] - mutants[\"mutant_start\"]\n",
    "    mutants[\"best_jaspar_name\"] = mutants.apply(lambda x:jaspar.name.loc[x.best_jaspar_id],axis = 1)\n",
    "    mutants[\"best_jaspar_genome_len\"] = mutants.apply(lambda x:len(jaspar.consensus.loc[x.best_jaspar_id]),axis = 1)\n",
    "    mutants[\"best_jaspar_motif_consensus\"]= mutants.best_jaspar_id.apply(lambda x: jaspar.loc[x].consensus)\n",
    "\n",
    "    mutants = mutants.set_index(mutants.apply(lambda x:x.wt_mu - x.mut_mu ,axis =1).rename(\"diffval\"),append=True)\\\n",
    "        .sort_index(level=1).reset_index().reset_index()\\\n",
    "        .set_index(\"index\").rename({\"level_0\":\"diffrank\"},axis=\"columns\")\n",
    "    return mutants\n",
    "\n",
    "\n",
    "\n",
    "exp_mutants = pd.concat([ \n",
    "    get_mutants(oligos_by_exp.loc[k],g.set_index(\"oligo\"),all_30bp_enhancer_sequences, pwm_scores_grid, pwm_indices_grid,jaspar)\n",
    "        for k,g in exp_enhancers.groupby(\"exp\")])\n",
    "pooled_mutants = get_mutants(oligos, pooled_enhancers.set_index(\"oligo\"), all_30bp_enhancer_sequences,pwm_scores_grid,pwm_indices_grid,jaspar)\n",
    "#pooled and split mutant files\n",
    "exp_mutants.to_csv(\"../out/0707_STARR_exp_mutants_simple.csv\")\n",
    "pooled_mutants.to_csv(\"../out/0707_STARR_pooled_mutants_simple.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mutants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECK READCOUNTS ####\n",
    "\n",
    "This part of the code is responsible for looking over experimental read counts.\n",
    "It is NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PARSERS ALL POSSIBLE ENHANCER SUBREGIONS\n",
    "readcounts_df = pd.read_csv(\"../out/0707_exp_readcounts.csv\")\n",
    "exp_tx_reads = pd.DataFrame(readcounts_df.dictionary_reads).set_index(readcounts_df.exp)\n",
    "exp_oligo_reads = pd.DataFrame(readcounts_df.oligo_reads).set_index(readcounts_df.exp)\n",
    "exp_sequencing_stats = pd.concat([readcounts_df.set_index(\"exp\")[[\"dictionary_reads\",\"oligo_reads\"]],oligo_exp_info[[\"n_bcs\",\"n_transcripts\",\"exp\"]].groupby(\"exp\").sum()],axis = 1)\\\n",
    "    .rename({\"n_bcs\":\"n_oligo_bcs\",\"n_transcripts\":\"n_tx_umis\",\"dictionary_reads\":\"tx_reads\"},axis=\"columns\")\n",
    "import os\n",
    "os.makedirs(\"../out/0708\")\n",
    "exp_sequencing_stats.to_csv(\"../out/0708/exp_sequencing_stats.csv\")\n",
    "p1 = sns.scatterplot(x=\"tx_reads\", y=\"n_tx_umis\",\n",
    "                     hue = exp_sequencing_stats.apply(lambda x: re.compile(\"BR[12]\").sub(\"\",x.name),axis=1), data = exp_sequencing_stats)\n",
    "p1.legend(loc=[1.1,0])\n",
    "\n",
    "print(f\"\"\"\n",
    "sequencing stats report:\n",
    "average number of tx umis per oligo: {(exp_sequencing_stats.n_tx_umis / exp_sequencing_stats.n_oligo_bcs).mean():.2f}\n",
    "average number of tx reads per oligo: {(exp_sequencing_stats.tx_reads / exp_sequencing_stats.n_oligo_bcs).mean():.2f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 ANALYSIS ##\n",
    "\n",
    "**CHOOSE A DIFFERENT SET OF REGIONS WITH FEWER FILTERS** \n",
    "\n",
    "Run a modified version of the above analysis, treating wild-type and non wt oligos seperately. Normalizing to an enrichment level of 1 in the pooled data, weight each experiment properly by total number of reads. For each experiment in the weighted pools, identify all adjacent pairs of WT oligos which are both expressed at a moderately enriched level. Looking at each pair in turn, evaluate whether (1) a single ablation, or (2) a double ablation candidate significantly diminishes expression, defining a significant reduction as one which diminishes expression to WT - std(local_wildtype) [check this]. Yield the following list of 60bp enhancer sequences and test for motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_60bp_subregions = pd.DataFrame()\n",
    "subregions = None\n",
    "for k,g in oligos_by_exp.groupby(level = \"exp\"):\n",
    "    wt_oligos = g.loc[g.mutant_num == 0] \n",
    "    mean_mu = wt_oligos.mu.mean()\n",
    "    enriched_mu = EXP_MODERATE_EXPR_MU\n",
    "    enriched_wt_oligos = wt_oligos.loc[wt_oligos.mu > enriched_mu]\n",
    "    enriched_wt_starts = enriched_oligos.starts.unique()\n",
    "    non_enriched_mutants = g.loc[(g.mu<enriched_mu) & (g.mutant_num > 0) & (g.starts.isin(enriched_wt_starts))]\n",
    "    motif_30bp_genome_start_proposals = pd.Series(non_enriched_mutants.mutant_start.unique()) + oligos_by_exp.gstart.min()\n",
    "    motif_60bp_subregions = motif_30bp_genome_start_proposals.apply(lambda x: chrseq[x - 15 : x+45])\n",
    "    if subregions is None:\n",
    "        subregions = pd.DataFrame(motif_60bp_subregions.rename(\"seq\")).assign(exp=k)\n",
    "    else:\n",
    "        subregions = pd.concat([subregions, pd.DataFrame(motif_60bp_subregions.rename(\"seq\")).assign(exp=k)],ignore_index = True)\n",
    "    \n",
    "#COMPUTE PWM SCORES FOR NEW REGIONS\n",
    "from Bio import Seq, Alphabet\n",
    "regions60  = pd.DataFrame(pd.Series(subregions.seq.unique()).rename(\"seq\")).reset_index().set_index(\"seq\").apply(lambda x: Seq.Seq(x.name,alphabet=Alphabet.IUPAC.IUPACUnambiguousDNA()),axis=1)\n",
    "pwm_r60_grid = jaspar.pssm.apply(lambda  pssm: regions60.apply(lambda x:  sorted(pssm.search(x),key=lambda x:-1*x[1])))\n",
    "pwm_r60_scores_grid = pwm_r60_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][1]).unstack().T\n",
    "pwm_r60_indices_grid = pwm_r60_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][0]).unstack().T\n",
    "pwm_r60_grid.to_csv(\"../out/0709_pwm_r60_grid0.csv\")\n",
    "\n",
    "#REORGANIZE PWM SCORES!\n",
    "# def get_r60(jaspar, pwm_r60_scores_grid,pwm_r60_indices_grid):\n",
    "rc = lambda x: \"\".join([{\"A\":\"T\",\"G\":\"C\",\"C\":\"G\",\"T\":\"A\"}[l] for l in x][::-1])\n",
    "r60_max_motif_ids = pd.DataFrame(pwm_r60_scores_grid.idxmax().rename(\"motif_id\"))\n",
    "r60_max_centroids = r60_max_motif_ids.motif_id.apply(lambda x: jaspar.km_centroid_jaspar_id.get(x)).rename(\"cluster_centroid\")\n",
    "r60_km_cluster_ids = r60_max_motif_ids.motif_id.apply(lambda x: jaspar.km_cluster_id.get(x)).rename(\"km_cluster_id\")\n",
    "r60_spec2_cluster_ids = r60_max_motif_ids.motif_id.apply(lambda x: jaspar.spec2_cluster_id.get(x)).rename(\"spec2_cluster_id\")\n",
    "r60_spec3_cluster_ids = r60_max_motif_ids.motif_id.apply(lambda x: jaspar.spec3_cluster_id.get(x)).rename(\"spec3_cluster_id\")\n",
    "r60_ms_cluster_ids = r60_max_motif_ids.motif_id.apply(lambda x: jaspar.ms_cluster_id.get(x)).rename(\"ms_cluster_id\")\n",
    "r60_motif_positions = r60_max_motif_ids.apply(lambda x: pwm_r60_indices_grid[x.name].loc[x].values[0],axis=1).rename(\"position\")\n",
    "r60_motif_scores = r60_max_motif_ids.apply(lambda x: pwm_r60_scores_grid[x.name].loc[x].values[0],axis=1).rename(\"score\")\n",
    "r60_motif_lens = r60_max_motif_ids.apply(lambda x: jaspar.loc[x][\"len\"].values[0],axis=1).rename(\"motif_len\")\n",
    "r60_motif_seqs = pd.DataFrame(r60_motif_positions.reset_index().apply(lambda x: x.seq[x.position:x.position+r60_motif_lens.loc[x.seq]]\n",
    "                                               if x.position >= 0 \n",
    "                                               else rc(x.seq[60+x.position:60+x.position+r60_motif_lens.loc[x.seq]]),axis=1)\\\n",
    "    .rename(\"motif_actual_seq\")).set_index(r60_motif_positions.index).motif_actual_seq\n",
    "r60 = pd.concat([r60_max_motif_ids,r60_max_centroids,\n",
    "             r60_km_cluster_ids,r60_ms_cluster_ids,r60_spec3_cluster_ids,r60_spec2_cluster_ids,\n",
    "             r60_motif_positions,\n",
    "             r60_motif_scores,r60_motif_lens,r60_motif_seqs],axis =1)\n",
    "\n",
    "match60_positions = r60.apply(lambda x: [e.start() for e in re.compile(x.name).finditer(chrseq[oligos.gstart.min():oligos.gend.max()])][0],axis=1)\n",
    "match_motif_positions = (match60_positions + (r60.position + ((r60.position < 0) * 60)))\n",
    "r60[\"motif_starts\"] = match_motif_positions\n",
    "\n",
    "r60.to_csv(\"../out/0709_r60.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EVALUATE OLIGO EXPRESSION LEVELS FOR MATCHES ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "motifs = r60.copy()\n",
    "motif_hits = r60.drop_duplicates(\"motif_starts\").reset_index(drop=True)\n",
    "readout = None\n",
    "\n",
    "for k,multiindexed_oligos in oligos_by_exp.groupby([\"exp_nm\",\"rep\"]):\n",
    "    these_oligos = multiindexed_oligos.reset_index(level=0)\n",
    "    #these_oligos = oligos\n",
    "    overlapping_oligo_starts = motifs.apply(lambda x: these_oligos.loc[(these_oligos.starts < x.motif_starts) & (these_oligos.starts > x.motif_starts - 120)].starts.unique(),axis =1)\n",
    "    overlapping_wt_oligos = overlapping_oligo_starts.apply(lambda x: these_oligos.loc[these_oligos.starts.isin(x) & (these_oligos.mutant_num == 0)].RefSeqID)\n",
    "    overlapping_mutant_oligos = motifs.apply(lambda x:these_oligos.loc[(these_oligos.mutant_start < (x.motif_starts+30)) & (these_oligos.mutant_start> (x.motif_starts - 30 ))].RefSeqID,axis=1)\n",
    "    motif_overlap_wt_refseqs = overlapping_wt_oligos.stack()\n",
    "    motif_overlap_mutant_refseqs = overlapping_mutant_oligos.stack()\n",
    "    \n",
    "    motif_mutant_mu = motif_overlap_mutant_refseqs.groupby(level=0).apply(lambda x:these_oligos.set_index(\"RefSeqID\").loc[x].mu.mean()).rename(\"mutant_mu\")\n",
    "    motif_wt_mu = motif_overlap_wt_refseqs.groupby(level=0).apply(lambda x:these_oligos.set_index(\"RefSeqID\").loc[x].mu.mean()).rename(\"wt_mu\")\n",
    "    motif_mutant_mu.index.name = \"motif_hitidx\"\n",
    "    motif_wt_mu.index.name = \"motif_hitidx\"\n",
    "    motifs.index.name = \"motif_hitidx\"\n",
    "    \n",
    "    muvals =  pd.concat([motif_mutant_mu,motif_wt_mu],axis=1)\n",
    "    muvals[\"exp_nm\"]=k[0]\n",
    "    muvals[\"rep\"]=k[1]\n",
    "    if readout is None:\n",
    "        readout = muvals\n",
    "    else:\n",
    "        readout = pd.concat([readout,muvals])\n",
    "\n",
    "readout = readout.join(motifs[[\"km_cluster_id\",\"ms_cluster_id\",\"spec2_cluster_id\",\"spec3_cluster_id\",\"motif_id\"]],on=\"motif_hitidx\")\n",
    "readout[\"rep\"] = readout.rep.astype(np.int32)\n",
    "readout.to_csv(\"../0709_readout.csv\")\n",
    "\n",
    "#MELT THE READ OUT\n",
    "melted = readout.reset_index().melt(value_name= \"mu_val\",var_name=\"mu_type\",value_vars = [\"mutant_mu\",\"wt_mu\"],id_vars=[\"exp_nm\",\"rep\",\"motif_id\",\"ms_cluster_id\",\"spec2_cluster_id\",\"spec3_cluster_id\",\"km_cluster_id\"])\n",
    "melted.to_csv(\"../0709_melted.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL GARBAGE?\n",
    "# #[TODO]: ENCAPSULATE AS A FUNCTION\n",
    "\n",
    "# motif_zscores  = (((((pwm_r60_scores_grid).T -  jaspar.pssm_score_mean.T )/ jaspar.pssm_score_std).T))\n",
    "# motif_zscores[motif_zscores < .75 ] = np.nan\n",
    "# motif_zscores_grid = motif_zscores\n",
    "# motif_max_zscores = motif_zscores_grid.max(axis=1)\n",
    "\n",
    "# possible_hits_zs = motif_max_zscores.loc[motif_max_zscores> .75].index\n",
    "# possible_hits_old =np.unique( r60_max_motif_ids.values)\n",
    "# all_names = jaspar.name.loc[list(set(possible_hits_old).union(possible_hits_zs))]\n",
    "# best_names = jaspar.name.loc[list(set(possible_hits_old).intersection(possible_hits_zs))]\n",
    "# pd.DataFrame(all_names).to_csv(\"../out/0708/all_tfs.csv\")\n",
    "# pd.DataFrame(best_names).to_csv(\"../out/0708/best_tfs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGURES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_exemplar_exp.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_names = [\"motif_id\", \"spec3_cluster_id\"]\n",
    "for clustering in clustering_names:\n",
    "    mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"mu_type\"]).mu_val.mean()\n",
    "    deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "    deltas_by_cluster = deltas_stacked.unstack(level=\"exp_nm\")\n",
    "    deltas_cluster_exp =  deltas_stacked.unstack(level=clustering).fillna(0)\n",
    "    motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "    deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "\n",
    "    from scipy.spatial import distance\n",
    "    from scipy.cluster import hierarchy\n",
    "\n",
    "    correlations = deltas_exemplar_exp\n",
    "    correlations_array = correlations\n",
    "\n",
    "    row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(correlations_array), method='average')\n",
    "\n",
    "    col_linkage = hierarchy.linkage(\n",
    "        distance.pdist(correlations_array.T), method='average')\n",
    "\n",
    "    p = sns.clustermap(correlations, row_linkage=row_linkage, col_linkage=col_linkage,\n",
    "                       #row_colors=np.arange(len(row_linkage))+1,col_colors=np.arange(len(col_linkage))+1,\n",
    "                       method=\"average\",\n",
    "                   figsize=(5,5))#, cmap=cmap)\n",
    "    \n",
    "\n",
    "    #p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_names = [\"motif_id\", \"spec3_cluster_id\"]\n",
    "for clustering in clustering_names:\n",
    "    mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"mu_type\"]).mu_val.mean()\n",
    "    deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "    deltas_by_cluster = deltas_stacked.unstack(level=\"exp_nm\")\n",
    "    deltas_cluster_exp =  deltas_stacked.unstack(level=clustering).fillna(0)\n",
    "    motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "    deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "\n",
    "    from scipy.spatial import distance\n",
    "    from scipy.cluster import hierarchy\n",
    "\n",
    "    correlations = deltas_exemplar_exp.T.corr()\n",
    "    correlations_array = correlations\n",
    "\n",
    "    row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(correlations_array), method='average')\n",
    "\n",
    "    col_linkage = hierarchy.linkage(\n",
    "        distance.pdist(correlations_array.T), method='average')\n",
    "\n",
    "    p = sns.clustermap(correlations, row_linkage=row_linkage, col_linkage=col_linkage,\n",
    "                       #row_colors=np.arange(len(row_linkage))+1,col_colors=np.arange(len(col_linkage))+1,\n",
    "                       method=\"average\",\n",
    "                   figsize=(5,5))#, cmap=cmap)\n",
    "    \n",
    "\n",
    "    #p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_names = [\"motif_id\", \"spec3_cluster_id\"]\n",
    "for clustering in clustering_names:\n",
    "    mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"mu_type\"]).mu_val.mean()\n",
    "    deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "    deltas_by_cluster = deltas_stacked.unstack(level=\"exp_nm\")\n",
    "    deltas_cluster_exp =  deltas_stacked.unstack(level=clustering).fillna(0)\n",
    "    motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "    deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "\n",
    "    from scipy.spatial import distance\n",
    "    from scipy.cluster import hierarchy\n",
    "\n",
    "    correlations = deltas_exemplar_exp\n",
    "    correlations_array = correlations\n",
    "\n",
    "    row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(correlations_array), method='average')\n",
    "\n",
    "    col_linkage = hierarchy.linkage(\n",
    "        distance.pdist(correlations_array.T), method='average')\n",
    "\n",
    "    p = sns.clustermap(correlations, row_linkage=row_linkage, col_linkage=col_linkage,\n",
    "                       #row_colors=np.arange(len(row_linkage))+1,col_colors=np.arange(len(col_linkage))+1,\n",
    "                       method=\"average\",\n",
    "                   figsize=(13, 13))#, cmap=cmap)\n",
    "    \n",
    "\n",
    "    #p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clustering in [\"motif_id\",\"spec3_cluster_id\",\"ms_cluster_id\", \"spec2_cluster_id\",]:\n",
    "    plt.figure()\n",
    "    f,subs = plt.subplots(1,2)\n",
    "    f.set_size_inches(12,3)\n",
    "    mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"mu_type\"]).mu_val.mean()\n",
    "    deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "    deltas_by_cluster = deltas_stacked.unstack(level=\"exp_nm\")\n",
    "    plt.sca(subs[0])\n",
    "    g = sns.lineplot(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "    #g._legend.remove()\n",
    "    g.legend().set_visible(False)\n",
    "    plt.sca(subs[1])\n",
    "    \n",
    "    sns.heatmap(data = deltas_stacked.unstack(level=0).fillna(0))\n",
    "    \n",
    "    #plt.legend(loc=[1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = \"motif_id\"\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,5)\n",
    "mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"mu_type\"]).mu_val.mean()\n",
    "deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "deltas_by_cluster = deltas_stacked.unstack(level=[\"exp_nm\"])\n",
    "#sns.heatmap(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "#plt.legend(loc=[1,0])a\n",
    "\n",
    "deltas_cluster_exp =  deltas_stacked.unstack(level=clustering).fillna(0)\n",
    "motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n",
    "ax = plt.gca()\n",
    "p1.fig.suptitle(\"STARRSeq log2 fold change in mutant vs. wildtype expression\\naveraged over all overlapping oligos, mutations and replicates\")\n",
    "\n",
    "\n",
    "clustering = \"motif_id\"\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,5)\n",
    "mus_by_cluster = melted.groupby([clustering,\"rep\",\"mu_type\"]).mu_val.mean()\n",
    "deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "deltas_by_cluster = deltas_stacked.unstack(level=[\"rep\"])\n",
    "#sns.heatmap(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "#plt.legend(loc=[1,0])a\n",
    "\n",
    "deltas_cluster_exp =  deltas_stacked.unstack(level=clustering).fillna(0)\n",
    "motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n",
    "ax = plt.gca()\n",
    "p1.fig.suptitle(\"STARRSeq log2 fold change in mutant vs. wildtype expression\\naveraged over all overlapping oligos, mutations and replicates\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clustering = \"motif_id\"\n",
    "plt.figure()\n",
    "\n",
    "mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"rep\",\"mu_type\"]).mu_val.mean()\n",
    "deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "deltas_unstacked_twice = (np.abs(deltas_stacked.unstack(level=\"rep\")[2] - deltas_stacked.unstack(level=\"rep\")[1])).rename(\"rep_change\")\n",
    "\n",
    "#deltas_by_cluster = deltas_unstacked_twice.unstack(level=[\"exp_nm\"])\n",
    "#sns.heatmap(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "#plt.legend(loc=[1,0])a\n",
    "\n",
    "deltas_cluster_exp =  deltas_unstacked_twice.unstack(level=clustering).fillna(0)\n",
    "motif_lookup = motifs.reset_index()[[\"motif_actual_seq\",clustering]].groupby(clustering).first()[\"motif_actual_seq\"]\n",
    "deltas_exemplar_exp = deltas_cluster_exp.rename(lambda x: motif_lookup.loc[x], axis=\"columns\")\n",
    "p1 = sns.clustermap(data =deltas_exemplar_exp)\n",
    "\n",
    "ax = plt.gca()\n",
    "p1.fig.suptitle(\"STARRSeq log2 change, difference between replicates\")\n",
    "\n",
    "#p1.set_xlabels(motif_lookup.)\n",
    "\n",
    "clustering = \"spec3_cluster_id\"\n",
    "plt.figure()\n",
    "mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"rep\",\"mu_type\"]).mu_val.mean()\n",
    "deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "deltas_by_cluster = deltas_stacked.unstack(level=[\"exp_nm\",\"rep\"])\n",
    "#sns.heatmap(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "#plt.legend(loc=[1,0])\n",
    "sns.clustermap(data = deltas_stacked.unstack(level=0).fillna(0))\n",
    "\n",
    "\n",
    "clustering = \"ms_cluster_id\"\n",
    "plt.figure()\n",
    "mus_by_cluster = melted.groupby([clustering,\"exp_nm\",\"rep\",\"mu_type\"]).mu_val.mean()\n",
    "deltas_stacked = (np.log2(mus_by_cluster.unstack(level=\"mu_type\").mutant_mu / mus_by_cluster.unstack(level=\"mu_type\").wt_mu).rename(\"mu_change\"))\n",
    "deltas_by_cluster = deltas_stacked.unstack(level=[\"exp_nm\",\"rep\"])\n",
    "#sns.heatmap(x=clustering, y=\"mu_change\", hue=\"exp_nm\", data = deltas_stacked.reset_index())\n",
    "#plt.legend(loc=[1,0])\n",
    "sns.clustermap(data = deltas_stacked.unstack(level=0).fillna(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.gcf()\n",
    "f.set_size_inches(14,6)\n",
    "\n",
    "n_colors = 256 # Use 256 colors for the diverging color palette\n",
    "palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\n",
    "color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n",
    "\n",
    "def value_to_color(val):\n",
    "    val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n",
    "    ind = int(val_position * (n_colors - 1)) # target index in the color palette\n",
    "    return palette[ind]\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(\n",
    "    y=r60_max_centroids,\n",
    "    x=range(len(r60_max_centroids)),\n",
    "    s=r60_motif_scores / max(r60_motif_scores)*50,\n",
    "    c=((r60_motif_scores / max(r60_motif_scores) - .5)*2).astype(float).apply(value_to_color), # Vector of square color values, mapped to color palette\n",
    "    marker='s'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
