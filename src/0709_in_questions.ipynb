{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lin's feedback ####\n",
    "In general, I noticed a cell type specific pattern, attached slide 1 is a correlation matrix I calculated based on the normalised_u. U2OS cell lines are standing out from all the other cell lines.  From the genome browser view in slide 2, you can see the cell type specific peaks highlighted in blue. And we know HCT116 cell line has much lower expression in APOBEC3B, therefore it is not surprising that in HCT116, the central peak at APOBEC3B intronic enhancer (highlighted in yellow) is lower and less distal peaks are observed.  \n",
    "\n",
    "\n",
    "I noticed that you are using “starts_by_mean_expression” to set up high and med cutoff, which is the mean value from all the experiments? And the further downsteam peak calling and motif identification is also based on this mean value? If that is the case, I think that might lower the statistics power in general. I would highly recommend to separate the transcripts by exp groups, and perform the downstream analysis independently. The priority will be U2OS_WT and DLD1_WT experiments, because these are two cell lines we chose for screening. And it also makes more sense to find out the correlation between MERA hit regions and enhancers/promoters defined by this assay in  U2OS_WT and DLD1_WT experiments. Following these two sets of wild type experiments, we can cross compare the peaks and motifs in U2OS vs DLD1 vs HCT116 WT cell lines, and figure out which peak is common, and which is cell type specific.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rich's feedback ####\n",
    "\n",
    "1.       Where are the promoters/enhancers (STARR-Seq peaks) as defined by this assay for each experiment? This will allow us to ask whether there is correlation between MERA hit regions and enhancers/promoters as defined by this assay.\n",
    "\n",
    "2.       Do any of the regions have differential activity in different experiments (normalized for technical differences between experiments like total reads)? This is especially interesting given Lin’s experimental setup that includes:\n",
    "\n",
    "    a.       Wild-type human cell lines with high native APOBEC3B expression (U2OS, DLD-1).  \n",
    "    b.      These cell lines with a knockout in NFKB, which she has found to be one driver of APOBEC3B expression.  \n",
    "    c.       Wild-type HCT116 cells which do not express high native APOBEC3B.  \n",
    "    d.      HCT116 cells treated with Gemcitabine, which potently activates APOBEC3B expresion.  \n",
    "\n",
    "3.       Can we identify motifs or collections thereof that are responsible for enhancer/promoter activity? This should be possible with ~30-bp resolution given the tiled deletion strategy.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printx(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import z1_save_oligos, z2_save_jaspar\n",
    "oligos,oligos_by_exp = z1_save_oligos.load_oligos()\n",
    "z2_save_jaspar.save_jaspar()\n",
    "jaspar = z2_save_jaspar.load_jaspar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOTIFS RESPONSIBLE FOR PROMOTER ACTIVITY ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GO TERM ENRICHMENT OF PROMOTER MOTIFS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Next, could you please further elaborate on what have you modified and how you get this wider initial motif list? [...] I noticed that you mentioned “To this point I have tried to work mainly with log ratios of oligo expression levels within experiments...”. So did you split the experiments up first and then perform peak calling and motif scanning? If so, could you please share with me a table of the peak regions (both high and med) with the absolute numbers of transcript umis observed / barcode in each experiment?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I made two changes -- first off, since we're specifically interested in finding differences between celltypes, I figured that we should be initially as permissive as possible in selecting mutants to create regions of interest which would be used for motif discovery. \n",
    "\n",
    "**CANDIDATE ENHANCER SUBREGION DISCOVERY**\n",
    "\n",
    "First off, I did start calling initial peaks in each sample separately. To do this, employed a simple normalization procedure filtering the roughly the top 5% of all unmutated oligos in each experiment into a pool of enriched oligos. Then, for each filtered oligo, I created a list of all mutants which repressed transcription, either (a) below the 95th percentile, or (b) below the wildtype by 1 stddev. Finally, to keep any mutant subregion as a candidate of interest I demanded that it appear in at least two samples, whether in two replicates of one experiment or two independent experiments. This filter had similar permissivity to the search we applied last time. Each of these mutants was associated with a 30bp mutation subregion which would be considered in a later step for motif discovery. \n",
    "\n",
    "**SUBREGION EXTENSION AND MOTIF DISCOVERY**  \n",
    "\n",
    "I joined each 30bp subregion with 15bp of flanking sequence on either sideto create 60 bp regions which would be used for motif discovery. For each 60bp subregion, I tried a de-novo and a database-guided approach to motif discovery. I figured we probably wouldn't have enough information for de novo discovery and that seems to be the case . We may return to this later. I don't think it will be productive yet.\n",
    "\n",
    "I performed pwm-based motif scoring using 547 motifs from the HOMER dataset For each motif, we would like to say whether a given enhancer has a significant hit for that motif. Additive scoring of log odds for a position weight matrix will be higher for longer motifs. To judge relative goodness-of-fit for motifs of different lengths, we need to determine the expected score and standard deviation of each motif in the background. From this, we will compute a zscore for each possible motif hit. From a background model of nucleotide content based upon the APOBEC region, we computed a log odds probability of each motif in each 60bp sequence. Whereas before, I took exactly one, top-scoring motif hit for each region, this time I set a threshold based the log-odds probability over the background. We chose an FDR-based threshold of .005, and accepted any motif hits within each subregion meeting that threshold\n",
    "\n",
    "To create the more permissive list of motifs, I merged this with the previously identified motifs. To create the stricter list of motifs, I took its intersection with this set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LOADS BIOLOGICAL MOTIFS AND SCANS ALL SUBREGIONS FOR OCCURENCES\n",
    "from pyfaidx import Fasta\n",
    "sequences_fa = Fasta('/cluster/bh0085/genomes/GRCh38.primary_assembly.genome.fa')\n",
    "chrseq = str(sequences_fa[\"chr22\"])\n",
    "all_60bp_subregions = pd.DataFrame()\n",
    "subregions = None\n",
    "for k,g in oligos_by_exp.groupby(level = \"exp\"):\n",
    "    print(k)\n",
    "    wt_oligos = g.loc[g.mutant_num == 0] \n",
    "\n",
    "    \n",
    "    enriched_mu = np.percentile(wt_oligos.exp_norm_mu,95)\n",
    "    enriched_wt_oligos = wt_oligos.loc[wt_oligos.exp_norm_mu > enriched_mu]\n",
    "    enriched_wt_starts = enriched_wt_oligos.starts.unique()\n",
    "    \n",
    "    g[\"wt_mu\"] = g.starts.apply(lambda x: (g.loc[(g[\"mutant_num\"] == 0) & (g[\"starts\"] == x)].exp_norm_mu.min()))\n",
    "    g_wt_enriched =  g.loc[g.starts.isin(enriched_wt_starts)]\n",
    "    non_enriched_mutants = g_wt_enriched.loc[ \n",
    "        (((g_wt_enriched.exp_norm_mu <enriched_mu) & (g_wt_enriched.mutant_num > 0)) |\n",
    "        (g_wt_enriched.exp_norm_mu < (g_wt_enriched.wt_mu - g_wt_enriched.exp_norm_mu.std()))) \n",
    "    ]\n",
    "    \n",
    "    motif_30bp_genome_start_proposals = pd.Series(non_enriched_mutants.mutant_start.unique()) + oligos_by_exp.gstart.min()\n",
    "    motif_60bp_subregions = motif_30bp_genome_start_proposals.apply(lambda x: chrseq[x - 15 : x+45])\n",
    "    if subregions is None:\n",
    "        subregions = pd.DataFrame(motif_60bp_subregions.rename(\"seq\")).assign(exp=k)\n",
    "    else:\n",
    "        subregions = pd.concat([subregions, pd.DataFrame(motif_60bp_subregions.rename(\"seq\")).assign(exp=k)],ignore_index = True)\n",
    "\n",
    "subregions = subregions.loc[subregions[[\"seq\"]].join(subregions.seq.value_counts().rename(\"seq_count\"),on=\"seq\").seq_count > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORE MOTIF REGIONS TO FIND ALL HITS WITHIN EACH SUBREGION IDENTIFIED ABOVE\n",
    "from Bio import Seq, Alphabet\n",
    "regions60  = pd.DataFrame(pd.Series(subregions.seq.unique()).rename(\"seq\")).reset_index().set_index(\"seq\").apply(lambda x: Seq.Seq(x.name,alphabet=Alphabet.IUPAC.IUPACUnambiguousDNA()),axis=1)\n",
    "pwm_r60_grid = jaspar.pssm.apply(lambda  pssm: regions60.apply(lambda x:  sorted(pssm.search(x),key=lambda x:-1*x[1])))\n",
    "pwm_r60_scores_grid = pwm_r60_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][1]).unstack().T\n",
    "pwm_r60_indices_grid = pwm_r60_grid.unstack().apply(lambda x: 0 if len(x)==0 else x[0][0]).unstack().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTER MOTIF SELECTIONS ACCORDING TO FDR RATE .005\n",
    "all_seq_motif_matches = (pwm_r60_scores_grid.T - jaspar.pssm_score_threshold + 1*jaspar.pssm_score_std).reset_index().melt(id_vars=[\"seq\"])\n",
    "seq_motif_matches = all_seq_motif_matches .loc[all_seq_motif_matches.value > 0]\n",
    "r60_max_motif_ids = seq_motif_matches[[\"jaspar_id\",\"seq\"]].rename({\"jaspar_id\":\"motif_id\"}) #pd.DataFrame(pwm_r60_scores_grid.idxmax().rename(\"motif_id\"))\n",
    "r60_max_motif_ids.to_csv(\"../data/0711_motif_manyregions_1std.csv\")\n",
    "\n",
    "\n",
    "#FILTER MOTIF SELECTIONS ACCORDING TO FDR RATE .005\n",
    "all_seq_motif_matches = (pwm_r60_scores_grid.T - jaspar.pssm_score_threshold + .5*jaspar.pssm_score_std).reset_index().melt(id_vars=[\"seq\"])\n",
    "seq_motif_matches = all_seq_motif_matches .loc[all_seq_motif_matches.value > 0]\n",
    "r60_max_motif_ids = seq_motif_matches[[\"jaspar_id\",\"seq\"]].rename({\"jaspar_id\":\"motif_id\"}) #pd.DataFrame(pwm_r60_scores_grid.idxmax().rename(\"motif_id\"))\n",
    "r60_max_motif_ids.to_csv(\"../data/0711_motif_manyregions_.5std.csv\")\n",
    "\n",
    "#FILTER MOTIF SELECTIONS ACCORDING TO FDR RATE .005\n",
    "all_seq_motif_matches = (pwm_r60_scores_grid.T - jaspar.pssm_score_threshold + 1.5*jaspar.pssm_score_std).reset_index().melt(id_vars=[\"seq\"])\n",
    "seq_motif_matches = all_seq_motif_matches .loc[all_seq_motif_matches.value > 0]\n",
    "r60_max_motif_ids = seq_motif_matches[[\"jaspar_id\",\"seq\"]].rename({\"jaspar_id\":\"motif_id\"}) #pd.DataFrame(pwm_r60_scores_grid.idxmax().rename(\"motif_id\"))\n",
    "r60_max_motif_ids.to_csv(\"../data/0711_motif_manyregions_1.5std.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/bh0085/anaconda27/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/cluster/bh0085/anaconda27/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  del sys.path[0]\n",
      "/cluster/bh0085/anaconda27/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CREATE A DATAFRAME WITH MOTIF CLUSTERING INFORMATION FOR EACH MOTIF IN EACH 60BP REGION OF INTEREST\n",
    "import re\n",
    "rc = lambda x: \"\".join([{\"A\":\"T\",\"G\":\"C\",\"C\":\"G\",\"T\":\"A\"}[l] for l in x][::-1])\n",
    "\n",
    "r60_max_centroids = r60_max_motif_ids.jaspar_id.apply(lambda x: jaspar.km_centroid_jaspar_id.get(x)).rename(\"cluster_centroid\")\n",
    "r60_km_cluster_ids = r60_max_motif_ids.jaspar_id.apply(lambda x: jaspar.km_cluster_id.get(x)).rename(\"km_cluster_id\")\n",
    "r60_spec2_cluster_ids = r60_max_motif_ids.jaspar_id.apply(lambda x: jaspar.spec2_cluster_id.get(x)).rename(\"spec2_cluster_id\")\n",
    "r60_spec3_cluster_ids = r60_max_motif_ids.jaspar_id.apply(lambda x: jaspar.spec3_cluster_id.get(x)).rename(\"spec3_cluster_id\")\n",
    "r60_ms_cluster_ids = r60_max_motif_ids.jaspar_id.apply(lambda x: jaspar.ms_cluster_id.get(x)).rename(\"ms_cluster_id\")\n",
    "r60_motif_positions = r60_max_motif_ids.apply(lambda x: pwm_r60_indices_grid[x.seq].loc[x].values[0],axis=1).rename(\"position\").astype(np.int32)\n",
    "r60_motif_positions = pd.concat([r60_motif_positions,r60_max_motif_ids.seq],axis=1)\n",
    "\n",
    "r60_motif_scores = r60_max_motif_ids.apply(lambda x: pwm_r60_scores_grid[x.seq].loc[x].values[0],axis=1).rename(\"score\")\n",
    "r60_motif_lens = r60_max_motif_ids.apply(lambda x: jaspar.loc[x][\"len\"].values[0],axis=1).rename(\"motif_len\")\n",
    "r60_motif_seqs = pd.DataFrame(r60_motif_positions.apply(lambda x: x.seq[int(x.position):int(x.position+r60_motif_lens.loc[x.name])]\n",
    "                                               if x.position >= 0 \n",
    "                                               else rc(x.seq[int(60+x.position):int(60+x.position+r60_motif_lens.loc[x.name])]),axis=1)\\\n",
    "    .rename(\"motif_actual_seq\")).set_index(r60_motif_positions.index).motif_actual_seq\n",
    "r60 = pd.concat([r60_max_motif_ids,r60_max_centroids,\n",
    "             r60_km_cluster_ids,r60_ms_cluster_ids,r60_spec3_cluster_ids,r60_spec2_cluster_ids,\n",
    "             r60_motif_positions[[\"position\"]],\n",
    "             r60_motif_scores,r60_motif_lens,r60_motif_seqs],axis =1)\n",
    "\n",
    "\n",
    "match60_positions = r60.apply(lambda x: [e.start() for e in re.compile(x.seq).finditer(chrseq[oligos.gstart.min():oligos.gend.max()])][0],axis=1)\n",
    "match_motif_positions = (match60_positions + (r60.position + ((r60.position < 0) * 60)))\n",
    "r60[\"motif_starts\"] = match_motif_positions\n",
    "r60.to_csv(\"../out/0709_r60_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'motif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-253247f0ee23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mreadout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmuvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mreadout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotifs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"km_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ms_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spec2_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spec3_cluster_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"motif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rep\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmelted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"mu_val\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mu_type\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"mutant_mu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wt_mu\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_nm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"motif\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ms_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spec2_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spec3_cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"km_cluster_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6813\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6814\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6815\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6817\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6828\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6829\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6830\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6832\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    527\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    528\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                     \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda27/envs/py3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1704\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'motif'"
     ]
    }
   ],
   "source": [
    "match60_positions = r60.apply(lambda x: [e.start() for e in re.compile(x.seq).finditer(chrseq[oligos.gstart.min():oligos.gend.max()])][0],axis=1)\n",
    "match_motif_positions = (match60_positions + (r60.position + ((r60.position < 0) * 60)))\n",
    "r60[\"motif_starts\"] = match_motif_positions\n",
    "\n",
    "motifs = r60.drop_duplicates([\"motif_starts\",\"jaspar_id\"]).reset_index(drop=True)\n",
    "readout = None\n",
    "\n",
    "for k,multiindexed_oligos in oligos_by_exp.groupby([\"exp_nm\",\"rep\"]):\n",
    "    these_oligos = multiindexed_oligos.reset_index(level=0)\n",
    "    #these_oligos = oligos\n",
    "    overlapping_oligo_starts = motifs.apply(lambda x: these_oligos.loc[(these_oligos.starts < x.motif_starts) & (these_oligos.starts > x.motif_starts - 120)].starts.unique(),axis =1)\n",
    "    overlapping_wt_oligos = overlapping_oligo_starts.apply(lambda x: these_oligos.loc[these_oligos.starts.isin(x) & (these_oligos.mutant_num == 0)].RefSeqID)\n",
    "    overlapping_mutant_oligos = motifs.apply(lambda x:these_oligos.loc[(these_oligos.mutant_start < (x.motif_starts+30)) & (these_oligos.mutant_start> (x.motif_starts - 30 ))].RefSeqID,axis=1)\n",
    "    motif_overlap_wt_refseqs = overlapping_wt_oligos.stack()\n",
    "    motif_overlap_mutant_refseqs = overlapping_mutant_oligos.stack()\n",
    "    \n",
    "    motif_mutant_mu = motif_overlap_mutant_refseqs.groupby(level=0).apply(lambda x:these_oligos.set_index(\"RefSeqID\").loc[x].mu.mean()).rename(\"mutant_mu\")\n",
    "    motif_wt_mu = motif_overlap_wt_refseqs.groupby(level=0).apply(lambda x:these_oligos.set_index(\"RefSeqID\").loc[x].mu.mean()).rename(\"wt_mu\")\n",
    "    motif_mutant_mu.index.name = \"motif_hit_id\"\n",
    "    motif_wt_mu.index.name = \"motif_hit_id\"\n",
    "    motifs.index.name = \"motif_hit_id\"\n",
    "    \n",
    "    muvals =  pd.concat([motif_mutant_mu,motif_wt_mu],axis=1)\n",
    "    muvals[\"exp_nm\"]=k[0]\n",
    "    muvals[\"rep\"]=k[1]\n",
    "    if readout is None:\n",
    "        readout = muvals\n",
    "    else:\n",
    "        readout = pd.concat([readout,muvals])\n",
    "\n",
    "readout = readout.join(motifs[[\"km_cluster_id\",\"ms_cluster_id\",\"spec2_cluster_id\",\"spec3_cluster_id\"]],on=\"motif\")\n",
    "readout[\"rep\"] = readout.rep.astype(np.int32)\n",
    "melted = readout.reset_index().melt(value_name= \"mu_val\",var_name=\"mu_type\",value_vars = [\"mutant_mu\",\"wt_mu\"],id_vars=[\"exp_nm\",\"rep\",\"motif\",\"ms_cluster_id\",\"spec2_cluster_id\",\"spec3_cluster_id\",\"km_cluster_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GO TERM ANALYSIS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "from goatools import obo_parser\n",
    "import wget\n",
    "import os, pandas as pd\n",
    "import os\n",
    "\n",
    "#THIS IS A WAY TO PARSE THE FULL GO TREE\n",
    "go_obo_url = 'http://purl.obolibrary.org/obo/go/go-basic.obo'\n",
    "data_folder = '/cluster/bh0085/data/go'\n",
    "\n",
    "# Check if we have the ./data directory already\n",
    "if(not os.path.isfile(data_folder)):\n",
    "    # Emulate mkdir -p (no error if folder exists)\n",
    "    try:\n",
    "        os.makedirs(data_folder)\n",
    "    except OSError as e:\n",
    "        if(e.errno != 17):\n",
    "            raise e\n",
    "else:\n",
    "    raise Exception('Data path (' + data_folder + ') exists as a file. '\n",
    "                   'Please rename, remove or change the desired location of the data path.')\n",
    "\n",
    "# Check if the file exists already\n",
    "if(not os.path.isfile(data_folder+'/go-basic.obo')):\n",
    "    go_obo = wget.download(go_obo_url, data_folder+'/go-basic.obo')\n",
    "else:\n",
    "    go_obo = data_folder+'/go-basic.obo'\n",
    "\n",
    "# Import the OBO parser from GOATools\n",
    "go = obo_parser.GODag(go_obo)\n",
    "\n",
    "\n",
    "\n",
    "# THIS IS A WAY TO PARSE THE GO RECORDS DIRECTLY INTO A PANDAS DATAFRAME\n",
    "\n",
    "from Bio.UniProt import GOA\n",
    "fopen = open(\"/cluster/bh0085/data/go/goa_human.gaf\")\n",
    "itr = GOA.gafiterator(fopen)\n",
    "records = list(itr)\n",
    "ontologies = pd.DataFrame.from_dict(records)   \n",
    "\n",
    "go_matches = jaspar.apply(lambda x: ontologies.loc[ontologies.DB_Object_Symbol.str.upper()==(x[\"name\"].upper())].GO_ID.T,axis=1).reset_index()\n",
    "go_melted = go_matches.melt(id_vars=\"jaspar_id\").dropna().reset_index(drop=True)\n",
    "semantic_terms = go_melted.value.apply(lambda term:go[term].name +\" \"+\\\n",
    "                                \" \".join([e.name for e in list(go[term].parents)]) +\\\n",
    "                                \" \".join([e.name for e in list(go[term].children)]) if term in go else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_terms.loc[semantic_terms.str.contains(\"hormone\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
