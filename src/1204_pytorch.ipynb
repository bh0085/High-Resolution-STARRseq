{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import z1_save_oligos, z2_save_jaspar, z3_transformations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Data processing frameworks\n",
    "import pandas as pd, numpy as np, scipy.stats as stats\n",
    "#Plotting & visualization\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "#Pytorch\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# FETCH DATA\n",
    "# note, this code will need to be run only once on a running colab instance.\n",
    "# once it has been run, you may comment it out.\n",
    "# !git clone https://github.com/gifford-lab/6884-antibodies-data.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LOAD JASPAR & OLIGO DATA\n",
    "jaspar = z2_save_jaspar.load_jaspar()\n",
    "oligos,oligos_by_exp = z1_save_oligos.load_oligos_plus()\n",
    "oligos_by_exp[\"analysis_group_key\"] = oligos_by_exp.exp.copy()\n",
    "all_obe = oligos_by_exp.reset_index().loc[lambda x:x.mutant_num<5].groupby([\"starts\",\"mutant_num\",\"analysis_group_key\"]).norm_mu.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'Sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d38396606752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_obe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'Sequences'"
     ]
    }
   ],
   "source": [
    "all_obe.Sequences.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_kmers(seq):\n",
    "    k = 4\n",
    "    kmer_keys =  [f\"{l1}{l2}{l3}{l4}\" for l1 in \"ATGC\" for l2  in \"ATGC\" for l3 in \"ATGC\" for l4 in \"ATGC\" ]\n",
    "    kdict = dict([[k,0] for k in kmer_keys] )\n",
    "    for i in range(len(seq) - len(kmer_keys[0])):\n",
    "        kdict[seq[i:i+4]] +=1 \n",
    "    return pd.Series(kdict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmer_counts = pd.Series(oligos_by_exp.Sequences.unique()).apply(lambda x: create_kmers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9903, 64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim =100, output_dim =1, dropout = False):\n",
    "        self.do_dropout = dropout\n",
    "        super(ANNModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity 1\n",
    "        # [STUDENTS] Add in a rectified linear unit to allow nonlinearity in the the first hidden layer transform.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "         # [STUDENTS] Add in a PyTorch fully connected linear maintaining the size of the hidden layer.\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.ln4 = nn.Linear(hidden_dim, hidden_dim)  \n",
    "\n",
    "        self.dp4 = nn.Dropout( 0.4)#, #50 % probability \n",
    "    \n",
    "        self.ln5 = nn.Linear(hidden_dim, hidden_dim)  \n",
    "    \n",
    "        \n",
    "        # Non-linearity 3\n",
    "        # [STUDENTS] Add in a ELU to allow nonlinearity in the final hidden layer transform.\n",
    "        self.elu3 = nn.ELU()\n",
    "        \n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        if(self.do_dropout):\n",
    "            #print (\"ADDING DROPOUT LAYER\")\n",
    "            #out = self.ln4(out)\n",
    "            out = self.dp4(out)\n",
    "            #out = self.ln5(out)\n",
    "            \n",
    "        # Non-linearity 2\n",
    "        # out = self.elu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will train the model, creating trained parameters. Choose a learning rate and you'll be ready to go!\n",
    "def run_model_training(model, train, test=None, batch_size=1000, n_iters=10000, num_epochs= 10,learning_rate=.005):\n",
    "  # SGD Optimizer\n",
    "  #[STUDENTS] Choose a learning rate for your optimizer\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "  train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "  test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  count = 0 \n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      #print (f'training loop: Running epoch {epoch} of {num_epochs}')\n",
    "      for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "          #images = np.reshape(images,(-1,20,30))\n",
    "          labels = np.reshape(labels,(-1,1)).float()\n",
    "\n",
    "          train_set = Variable(images.view(-1,64*4).float())#-1, 28*28))\n",
    "          labels = Variable(labels)\n",
    "          \n",
    "          # Clear gradients\n",
    "          optimizer.zero_grad()\n",
    "          \n",
    "          # Forward propagation\n",
    "          outputs = model(train_set)\n",
    "          \n",
    "          # Calculate MSE loss\n",
    "          loss_func = torch.nn.MSELoss()\n",
    "          loss = loss_func(outputs, labels) \n",
    "\n",
    "          # Calculating gradients\n",
    "          loss.backward()\n",
    "          \n",
    "          # Update parameters\n",
    "          optimizer.step()\n",
    "\n",
    "          count +=1;\n",
    "          if count %100 == 0: print('Training set loss--iteration: {}  Loss: {}'.format(count, loss.data))    \n",
    "\n",
    "          # [STUDENTS / OPTIONAL] set up a conditional print statement which \n",
    "          # tests the accuracy of the current trained model on testing data\n",
    "          # in order to judge the correct learning rate. Is your model performance\n",
    "          # improving in each epoch?\n",
    "          # \n",
    "          # for images, labels in test_loader: [...]\n",
    "              \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idxs = np.random.choice(range(len(features)),200)\n",
    "train_idxs = np.array(list(set(range(len(features))).symmetric_difference(test_idxs)))\n",
    "all_features = kmer_counts.sort_index()\n",
    "all_targets = oligos_by_exp.groupby(\"Sequences\").mu.mean().sort_index()\n",
    "\n",
    "train_features = all_features.iloc[train_idxs]\n",
    "train_targets = all_targets.iloc[train_idxs]\n",
    "test_features = all_features.iloc[test_idxs]\n",
    "test_targets = all_targets.iloc[test_idxs]\n",
    "\n",
    "featuresTrainRandomSplit = torch.from_numpy(train_features.values).type(torch.FloatTensor)\n",
    "targetsTrainRandomSplit = torch.from_numpy(train_targets.values).type(torch.FloatTensor)\n",
    "\n",
    "featuresTestRandomSplit = torch.from_numpy(test_features.values).type(torch.FloatTensor)\n",
    "targetsTestRandomSplit = torch.from_numpy(test_targets.values).type(torch.FloatTensor)\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(featuresTrainRandomSplit,targetsTrainRandomSplit)\n",
    "test_set = torch.utils.data.TensorDataset(featuresTestRandomSplit,targetsTestRandomSplit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss--iteration: 100  Loss: 8.609663963317871\n",
      "Training set loss--iteration: 200  Loss: 1.1950626373291016\n",
      "Training set loss--iteration: 300  Loss: 1.4607475996017456\n",
      "Training set loss--iteration: 400  Loss: 1.2533165216445923\n",
      "Training set loss--iteration: 500  Loss: 1.2155252695083618\n",
      "Training set loss--iteration: 600  Loss: 4.1617326736450195\n",
      "Training set loss--iteration: 700  Loss: 1.9474092721939087\n",
      "Training set loss--iteration: 800  Loss: 0.4408220052719116\n",
      "Training set loss--iteration: 900  Loss: 1.237488031387329\n",
      "Training set loss--iteration: 1000  Loss: 1.0035953521728516\n",
      "Training set loss--iteration: 1100  Loss: 2.0024008750915527\n",
      "Training set loss--iteration: 1200  Loss: 0.2852460443973541\n",
      "Training set loss--iteration: 1300  Loss: 1.5948669910430908\n",
      "Training set loss--iteration: 1400  Loss: 1.909821629524231\n",
      "Training set loss--iteration: 1500  Loss: 0.3636046350002289\n",
      "Training set loss--iteration: 1600  Loss: 0.5962837338447571\n",
      "Training set loss--iteration: 1700  Loss: 2.715304136276245\n",
      "Training set loss--iteration: 1800  Loss: 0.868419885635376\n",
      "Training set loss--iteration: 1900  Loss: 0.3851487934589386\n",
      "Training set loss--iteration: 2000  Loss: 0.2047673463821411\n",
      "Training set loss--iteration: 2100  Loss: 0.4219268262386322\n",
      "Training set loss--iteration: 2200  Loss: 0.2393767088651657\n",
      "Training set loss--iteration: 2300  Loss: 0.45518434047698975\n",
      "Training set loss--iteration: 2400  Loss: 0.18310216069221497\n",
      "Training set loss--iteration: 2500  Loss: 0.16939494013786316\n",
      "Training set loss--iteration: 2600  Loss: 0.30559080839157104\n",
      "Training set loss--iteration: 2700  Loss: 0.22242572903633118\n",
      "Training set loss--iteration: 2800  Loss: 0.16491973400115967\n",
      "Training set loss--iteration: 2900  Loss: 0.09680933505296707\n",
      "Training set loss--iteration: 3000  Loss: 0.13424213230609894\n",
      "Training set loss--iteration: 3100  Loss: 0.0886540487408638\n",
      "Training set loss--iteration: 3200  Loss: 0.08646472543478012\n",
      "Training set loss--iteration: 3300  Loss: 0.06965979933738708\n",
      "Training set loss--iteration: 3400  Loss: 0.08277177065610886\n",
      "Training set loss--iteration: 3500  Loss: 0.10796143859624863\n",
      "Training set loss--iteration: 3600  Loss: 0.027367813512682915\n",
      "Training set loss--iteration: 3700  Loss: 0.021093210205435753\n",
      "Training set loss--iteration: 3800  Loss: 0.061483412981033325\n",
      "Training set loss--iteration: 3900  Loss: 0.06088215857744217\n",
      "Training set loss--iteration: 4000  Loss: 0.023876044899225235\n",
      "Training set loss--iteration: 4100  Loss: 0.023132389411330223\n",
      "Training set loss--iteration: 4200  Loss: 0.00985859613865614\n",
      "Training set loss--iteration: 4300  Loss: 0.0660543143749237\n",
      "Training set loss--iteration: 4400  Loss: 0.00928905513137579\n",
      "Training set loss--iteration: 4500  Loss: 0.029715752229094505\n",
      "Training set loss--iteration: 4600  Loss: 0.018515311181545258\n",
      "Training set loss--iteration: 4700  Loss: 0.01617829129099846\n",
      "Training set loss--iteration: 4800  Loss: 0.016354555264115334\n",
      "Training set loss--iteration: 4900  Loss: 0.01288683246821165\n",
      "Training set loss--iteration: 5000  Loss: 0.04161769524216652\n",
      "Training set loss--iteration: 5100  Loss: 0.004973495379090309\n",
      "Training set loss--iteration: 5200  Loss: 0.0480063296854496\n",
      "Training set loss--iteration: 5300  Loss: 0.016042914241552353\n",
      "Training set loss--iteration: 5400  Loss: 0.1737600862979889\n",
      "Training set loss--iteration: 5500  Loss: 0.01650918461382389\n",
      "Training set loss--iteration: 5600  Loss: 0.02212054468691349\n",
      "Training set loss--iteration: 5700  Loss: 0.014605747535824776\n",
      "Training set loss--iteration: 5800  Loss: 0.0070217205211520195\n",
      "Training set loss--iteration: 5900  Loss: 0.004150635562837124\n",
      "Training set loss--iteration: 6000  Loss: 0.01616727188229561\n",
      "Training set loss--iteration: 6100  Loss: 0.004691466223448515\n",
      "Training set loss--iteration: 6200  Loss: 0.012258395552635193\n",
      "Training set loss--iteration: 6300  Loss: 0.009045498445630074\n",
      "Training set loss--iteration: 6400  Loss: 0.007436146028339863\n",
      "Training set loss--iteration: 6500  Loss: 0.00742384884506464\n",
      "Training set loss--iteration: 6600  Loss: 0.010793430730700493\n",
      "Training set loss--iteration: 6700  Loss: 0.023519493639469147\n",
      "Training set loss--iteration: 6800  Loss: 0.015091550536453724\n",
      "Training set loss--iteration: 6900  Loss: 0.0026109665632247925\n",
      "Training set loss--iteration: 7000  Loss: 0.009801305830478668\n",
      "Training set loss--iteration: 7100  Loss: 0.00603166688233614\n",
      "Training set loss--iteration: 7200  Loss: 0.0051771472208201885\n",
      "Training set loss--iteration: 7300  Loss: 0.00641363300383091\n",
      "Training set loss--iteration: 7400  Loss: 0.010973166674375534\n",
      "Training set loss--iteration: 7500  Loss: 0.005098043475300074\n",
      "Training set loss--iteration: 7600  Loss: 0.005786985158920288\n",
      "Training set loss--iteration: 7700  Loss: 0.005384483840316534\n",
      "Training set loss--iteration: 7800  Loss: 0.013230865821242332\n",
      "Training set loss--iteration: 7900  Loss: 0.0048174322582781315\n",
      "Training set loss--iteration: 8000  Loss: 0.00591604458168149\n",
      "Training set loss--iteration: 8100  Loss: 0.0034700720570981503\n",
      "Training set loss--iteration: 8200  Loss: 0.003812389448285103\n",
      "Training set loss--iteration: 8300  Loss: 0.006250651553273201\n",
      "Training set loss--iteration: 8400  Loss: 0.013642228208482265\n",
      "Training set loss--iteration: 8500  Loss: 0.010195713490247726\n",
      "Training set loss--iteration: 8600  Loss: 0.004475128371268511\n",
      "Training set loss--iteration: 8700  Loss: 0.005750992335379124\n",
      "Training set loss--iteration: 8800  Loss: 0.003870982676744461\n",
      "Training set loss--iteration: 8900  Loss: 0.005305230151861906\n",
      "Training set loss--iteration: 9000  Loss: 0.01034239400178194\n",
      "Training set loss--iteration: 9100  Loss: 0.009573222137987614\n",
      "Training set loss--iteration: 9200  Loss: 0.004309831652790308\n",
      "Training set loss--iteration: 9300  Loss: 0.003385003423318267\n",
      "Training set loss--iteration: 9400  Loss: 0.0013496013125404716\n",
      "Training set loss--iteration: 9500  Loss: 0.0027887600008398294\n",
      "Training set loss--iteration: 9600  Loss: 0.007625945843756199\n",
      "Training set loss--iteration: 9700  Loss: 0.0030515342950820923\n",
      "Training set loss--iteration: 9800  Loss: 0.005800455342978239\n",
      "Training set loss--iteration: 9900  Loss: 0.00764260720461607\n",
      "Training set loss--iteration: 10000  Loss: 0.004933810327202082\n",
      "Training set loss--iteration: 10100  Loss: 0.0064943828620016575\n",
      "Training set loss--iteration: 10200  Loss: 0.003254706971347332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-d660d46546c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_model_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-207-5e331e119332>\u001b[0m in \u001b[0;36mrun_model_training\u001b[0;34m(model, train, test, batch_size, n_iters, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0;31m# Calculate MSE loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-118592a36939>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Linear function 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 50\n",
    "n_iters = 15000\n",
    "#create model & training set\n",
    "input_dim = 64* 4\n",
    "num_epochs = int(n_iters / (len(featuresTrainRandomSplit) / batch_size))\n",
    "\n",
    "model = ANNModel(input_dim, hidden_dim = 15, output_dim = 1)\n",
    "#run training\n",
    "run_model_training(model, train_set,test = test_set, batch_size=batch_size, n_iters=n_iters, num_epochs=num_epochs, learning_rate = .002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss--iteration: 100  Loss: 7.350094318389893\n",
      "Training set loss--iteration: 200  Loss: 0.8189486861228943\n",
      "Training set loss--iteration: 300  Loss: 1.1590336561203003\n",
      "Training set loss--iteration: 400  Loss: 7.612275123596191\n",
      "Training set loss--iteration: 500  Loss: 0.9006824493408203\n",
      "Training set loss--iteration: 600  Loss: 2.7971723079681396\n",
      "Training set loss--iteration: 700  Loss: 10.151224136352539\n",
      "Training set loss--iteration: 800  Loss: 6.198485851287842\n",
      "Training set loss--iteration: 900  Loss: 1.8913052082061768\n",
      "Training set loss--iteration: 1000  Loss: 3.1236040592193604\n",
      "Training set loss--iteration: 1100  Loss: 1.087859869003296\n",
      "Training set loss--iteration: 1200  Loss: 4.6946868896484375\n",
      "Training set loss--iteration: 1300  Loss: 1.8925706148147583\n",
      "Training set loss--iteration: 1400  Loss: 0.5458353161811829\n",
      "Training set loss--iteration: 1500  Loss: 3.57850980758667\n",
      "Training set loss--iteration: 1600  Loss: 2.5873961448669434\n",
      "Training set loss--iteration: 1700  Loss: 0.8757238388061523\n",
      "Training set loss--iteration: 1800  Loss: 1.984296441078186\n",
      "Training set loss--iteration: 1900  Loss: 0.8719979524612427\n",
      "Training set loss--iteration: 2000  Loss: 1.850500464439392\n",
      "Training set loss--iteration: 2100  Loss: 0.8903579711914062\n",
      "Training set loss--iteration: 2200  Loss: 7.6501898765563965\n",
      "Training set loss--iteration: 2300  Loss: 4.344428062438965\n",
      "Training set loss--iteration: 2400  Loss: 4.883108615875244\n",
      "Training set loss--iteration: 2500  Loss: 3.634202003479004\n",
      "Training set loss--iteration: 2600  Loss: 0.7569005489349365\n",
      "Training set loss--iteration: 2700  Loss: 7.482322692871094\n",
      "Training set loss--iteration: 2800  Loss: 1.3585197925567627\n",
      "Training set loss--iteration: 2900  Loss: 4.244035720825195\n",
      "Training set loss--iteration: 3000  Loss: 3.1413493156433105\n",
      "Training set loss--iteration: 3100  Loss: 3.944546699523926\n",
      "Training set loss--iteration: 3200  Loss: 1.2892590761184692\n",
      "Training set loss--iteration: 3300  Loss: 1.5048621892929077\n",
      "Training set loss--iteration: 3400  Loss: 5.874298095703125\n",
      "Training set loss--iteration: 3500  Loss: 0.6697632074356079\n",
      "Training set loss--iteration: 3600  Loss: 1.3570927381515503\n",
      "Training set loss--iteration: 3700  Loss: 1.2450529336929321\n",
      "Training set loss--iteration: 3800  Loss: 0.7971563339233398\n",
      "Training set loss--iteration: 3900  Loss: 1.0262004137039185\n",
      "Training set loss--iteration: 4000  Loss: 0.4815603792667389\n",
      "Training set loss--iteration: 4100  Loss: 0.3474201261997223\n",
      "Training set loss--iteration: 4200  Loss: 0.5749759674072266\n",
      "Training set loss--iteration: 4300  Loss: 0.33357059955596924\n",
      "Training set loss--iteration: 4400  Loss: 1.4930198192596436\n",
      "Training set loss--iteration: 4500  Loss: 1.5488029718399048\n",
      "Training set loss--iteration: 4600  Loss: 0.23867952823638916\n",
      "Training set loss--iteration: 4700  Loss: 0.32525354623794556\n",
      "Training set loss--iteration: 4800  Loss: 1.5882068872451782\n",
      "Training set loss--iteration: 4900  Loss: 0.2748911678791046\n",
      "Training set loss--iteration: 5000  Loss: 0.3023439943790436\n",
      "Training set loss--iteration: 5100  Loss: 0.24964408576488495\n",
      "Training set loss--iteration: 5200  Loss: 0.6378567814826965\n",
      "Training set loss--iteration: 5300  Loss: 1.31674325466156\n",
      "Training set loss--iteration: 5400  Loss: 0.3345964252948761\n",
      "Training set loss--iteration: 5500  Loss: 0.13656951487064362\n",
      "Training set loss--iteration: 5600  Loss: 2.440485954284668\n",
      "Training set loss--iteration: 5700  Loss: 0.43019384145736694\n",
      "Training set loss--iteration: 5800  Loss: 0.382922887802124\n",
      "Training set loss--iteration: 5900  Loss: 0.19830210506916046\n",
      "Training set loss--iteration: 6000  Loss: 1.0964672565460205\n",
      "Training set loss--iteration: 6100  Loss: 0.20815753936767578\n",
      "Training set loss--iteration: 6200  Loss: 3.381997585296631\n",
      "Training set loss--iteration: 6300  Loss: 0.21864961087703705\n",
      "Training set loss--iteration: 6400  Loss: 0.2342960089445114\n",
      "Training set loss--iteration: 6500  Loss: 0.3680398464202881\n",
      "Training set loss--iteration: 6600  Loss: 7.617086410522461\n",
      "Training set loss--iteration: 6700  Loss: 0.14453363418579102\n",
      "Training set loss--iteration: 6800  Loss: 0.7276186943054199\n",
      "Training set loss--iteration: 6900  Loss: 0.23087260127067566\n",
      "Training set loss--iteration: 7000  Loss: 0.24854271113872528\n",
      "Training set loss--iteration: 7100  Loss: 0.15491384267807007\n",
      "Training set loss--iteration: 7200  Loss: 0.6553815007209778\n",
      "Training set loss--iteration: 7300  Loss: 0.1926167607307434\n",
      "Training set loss--iteration: 7400  Loss: 0.41204309463500977\n",
      "Training set loss--iteration: 7500  Loss: 0.12475019693374634\n",
      "Training set loss--iteration: 7600  Loss: 0.21035100519657135\n",
      "Training set loss--iteration: 7700  Loss: 0.370859295129776\n",
      "Training set loss--iteration: 7800  Loss: 0.5440962910652161\n",
      "Training set loss--iteration: 7900  Loss: 1.6093168258666992\n",
      "Training set loss--iteration: 8000  Loss: 0.21335096657276154\n",
      "Training set loss--iteration: 8100  Loss: 0.24525649845600128\n",
      "Training set loss--iteration: 8200  Loss: 0.8124485611915588\n",
      "Training set loss--iteration: 8300  Loss: 0.238404780626297\n",
      "Training set loss--iteration: 8400  Loss: 0.09550853818655014\n",
      "Training set loss--iteration: 8500  Loss: 0.1547047346830368\n",
      "Training set loss--iteration: 8600  Loss: 0.14580264687538147\n",
      "Training set loss--iteration: 8700  Loss: 0.1971510946750641\n",
      "Training set loss--iteration: 8800  Loss: 0.1518206000328064\n",
      "Training set loss--iteration: 8900  Loss: 0.3399730324745178\n",
      "Training set loss--iteration: 9000  Loss: 0.1184244230389595\n",
      "Training set loss--iteration: 9100  Loss: 0.3917911648750305\n",
      "Training set loss--iteration: 9200  Loss: 0.11314158886671066\n",
      "Training set loss--iteration: 9300  Loss: 0.2567862868309021\n",
      "Training set loss--iteration: 9400  Loss: 0.09992531687021255\n",
      "Training set loss--iteration: 9500  Loss: 0.07834979891777039\n",
      "Training set loss--iteration: 9600  Loss: 1.4332492351531982\n",
      "Training set loss--iteration: 9700  Loss: 0.15436089038848877\n",
      "Training set loss--iteration: 9800  Loss: 0.07778351753950119\n",
      "Training set loss--iteration: 9900  Loss: 0.05629747733473778\n",
      "Training set loss--iteration: 10000  Loss: 0.13454215228557587\n",
      "Training set loss--iteration: 10100  Loss: 0.25462743639945984\n",
      "Training set loss--iteration: 10200  Loss: 1.2315037250518799\n",
      "Training set loss--iteration: 10300  Loss: 0.09876353293657303\n",
      "Training set loss--iteration: 10400  Loss: 0.19002799689769745\n",
      "Training set loss--iteration: 10500  Loss: 0.10832970589399338\n",
      "Training set loss--iteration: 10600  Loss: 0.15721137821674347\n",
      "Training set loss--iteration: 10700  Loss: 0.12467680871486664\n",
      "Training set loss--iteration: 10800  Loss: 0.3000350594520569\n",
      "Training set loss--iteration: 10900  Loss: 0.11943024396896362\n",
      "Training set loss--iteration: 11000  Loss: 0.13989117741584778\n",
      "Training set loss--iteration: 11100  Loss: 0.18776841461658478\n",
      "Training set loss--iteration: 11200  Loss: 0.0821509137749672\n",
      "Training set loss--iteration: 11300  Loss: 0.061536408960819244\n",
      "Training set loss--iteration: 11400  Loss: 0.08857794851064682\n",
      "Training set loss--iteration: 11500  Loss: 0.0890335738658905\n",
      "Training set loss--iteration: 11600  Loss: 1.7060315608978271\n",
      "Training set loss--iteration: 11700  Loss: 0.14503087103366852\n",
      "Training set loss--iteration: 11800  Loss: 0.7591428160667419\n",
      "Training set loss--iteration: 11900  Loss: 0.15688100457191467\n",
      "Training set loss--iteration: 12000  Loss: 0.24535143375396729\n",
      "Training set loss--iteration: 12100  Loss: 0.12644577026367188\n",
      "Training set loss--iteration: 12200  Loss: 0.11795056611299515\n",
      "Training set loss--iteration: 12300  Loss: 0.17107892036437988\n",
      "Training set loss--iteration: 12400  Loss: 0.11105180531740189\n",
      "Training set loss--iteration: 12500  Loss: 1.0484042167663574\n",
      "Training set loss--iteration: 12600  Loss: 0.23537537455558777\n",
      "Training set loss--iteration: 12700  Loss: 0.059937652200460434\n",
      "Training set loss--iteration: 12800  Loss: 0.4996906816959381\n",
      "Training set loss--iteration: 12900  Loss: 0.12692013382911682\n",
      "Training set loss--iteration: 13000  Loss: 0.07967395335435867\n",
      "Training set loss--iteration: 13100  Loss: 0.24544857442378998\n",
      "Training set loss--iteration: 13200  Loss: 1.4784858226776123\n",
      "Training set loss--iteration: 13300  Loss: 0.12264598160982132\n",
      "Training set loss--iteration: 13400  Loss: 0.03525134176015854\n",
      "Training set loss--iteration: 13500  Loss: 0.1507222056388855\n",
      "Training set loss--iteration: 13600  Loss: 0.03561960160732269\n",
      "Training set loss--iteration: 13700  Loss: 0.069099560379982\n",
      "Training set loss--iteration: 13800  Loss: 1.2547444105148315\n",
      "Training set loss--iteration: 13900  Loss: 0.10420548915863037\n",
      "Training set loss--iteration: 14000  Loss: 0.049221619963645935\n",
      "Training set loss--iteration: 14100  Loss: 0.6304931640625\n",
      "Training set loss--iteration: 14200  Loss: 0.6013776659965515\n",
      "Training set loss--iteration: 14300  Loss: 0.10068918764591217\n",
      "Training set loss--iteration: 14400  Loss: 0.46494555473327637\n",
      "Training set loss--iteration: 14500  Loss: 0.1476530283689499\n",
      "Training set loss--iteration: 14600  Loss: 0.08647915720939636\n",
      "Training set loss--iteration: 14700  Loss: 0.5597857236862183\n",
      "Training set loss--iteration: 14800  Loss: 0.13153018057346344\n",
      "Training set loss--iteration: 14900  Loss: 0.2624850571155548\n",
      "Training set loss--iteration: 15000  Loss: 0.07947486639022827\n",
      "Training set loss--iteration: 15100  Loss: 0.12263990938663483\n",
      "Training set loss--iteration: 15200  Loss: 0.28860679268836975\n",
      "Training set loss--iteration: 15300  Loss: 0.16818758845329285\n",
      "Training set loss--iteration: 15400  Loss: 0.049235180020332336\n",
      "Training set loss--iteration: 15500  Loss: 0.1061224713921547\n",
      "Training set loss--iteration: 15600  Loss: 0.08999988436698914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 50\n",
    "n_iters = 15000\n",
    "#create model & training set\n",
    "input_dim = 64*4\n",
    "num_epochs = int(n_iters / (len(featuresTrainRandomSplit) / batch_size))\n",
    "\n",
    "\n",
    "model_dropout2 = ANNModel(input_dim, hidden_dim =15, output_dim = 1, dropout = True)\n",
    "#run training\n",
    "run_model_training(model_dropout2, train_set,test = test_set, batch_size=batch_size, n_iters=n_iters, num_epochs=num_epochs, learning_rate = .001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss--iteration: 100  Loss: 6.326693534851074\n",
      "Training set loss--iteration: 200  Loss: 2.065912961959839\n",
      "Training set loss--iteration: 300  Loss: 1.0785131454467773\n",
      "Training set loss--iteration: 400  Loss: 1.8286206722259521\n",
      "Training set loss--iteration: 500  Loss: 8.759082794189453\n",
      "Training set loss--iteration: 600  Loss: 1.335742473602295\n",
      "Training set loss--iteration: 700  Loss: 0.575649619102478\n",
      "Training set loss--iteration: 800  Loss: 9.370569229125977\n",
      "Training set loss--iteration: 900  Loss: 0.6284834742546082\n",
      "Training set loss--iteration: 1000  Loss: 0.7863384485244751\n",
      "Training set loss--iteration: 1100  Loss: 0.6744176149368286\n",
      "Training set loss--iteration: 1200  Loss: 0.40927404165267944\n",
      "Training set loss--iteration: 1300  Loss: 0.4449608325958252\n",
      "Training set loss--iteration: 1400  Loss: 2.5515599250793457\n",
      "Training set loss--iteration: 1500  Loss: 4.610994338989258\n",
      "Training set loss--iteration: 1600  Loss: 0.49084922671318054\n",
      "Training set loss--iteration: 1700  Loss: 1.672143578529358\n",
      "Training set loss--iteration: 1800  Loss: 0.42478132247924805\n",
      "Training set loss--iteration: 1900  Loss: 0.9966477155685425\n",
      "Training set loss--iteration: 2000  Loss: 6.356777191162109\n",
      "Training set loss--iteration: 2100  Loss: 0.8450533151626587\n",
      "Training set loss--iteration: 2200  Loss: 4.006007194519043\n",
      "Training set loss--iteration: 2300  Loss: 0.47651898860931396\n",
      "Training set loss--iteration: 2400  Loss: 1.130807876586914\n",
      "Training set loss--iteration: 2500  Loss: 1.4578009843826294\n",
      "Training set loss--iteration: 2600  Loss: 0.6371691226959229\n",
      "Training set loss--iteration: 2700  Loss: 0.2600211799144745\n",
      "Training set loss--iteration: 2800  Loss: 2.085343837738037\n",
      "Training set loss--iteration: 2900  Loss: 0.10627651959657669\n",
      "Training set loss--iteration: 3000  Loss: 0.3204860985279083\n",
      "Training set loss--iteration: 3100  Loss: 0.4690491557121277\n",
      "Training set loss--iteration: 3200  Loss: 4.865252494812012\n",
      "Training set loss--iteration: 3300  Loss: 0.15783952176570892\n",
      "Training set loss--iteration: 3400  Loss: 0.24037384986877441\n",
      "Training set loss--iteration: 3500  Loss: 0.23486165702342987\n",
      "Training set loss--iteration: 3600  Loss: 0.17018353939056396\n",
      "Training set loss--iteration: 3700  Loss: 0.531353771686554\n",
      "Training set loss--iteration: 3800  Loss: 0.07414540648460388\n",
      "Training set loss--iteration: 3900  Loss: 0.2095908522605896\n",
      "Training set loss--iteration: 4000  Loss: 0.26298123598098755\n",
      "Training set loss--iteration: 4100  Loss: 0.4326716363430023\n",
      "Training set loss--iteration: 4200  Loss: 0.16218478977680206\n",
      "Training set loss--iteration: 4300  Loss: 0.14192551374435425\n",
      "Training set loss--iteration: 4400  Loss: 0.23058851063251495\n",
      "Training set loss--iteration: 4500  Loss: 0.09072715044021606\n",
      "Training set loss--iteration: 4600  Loss: 0.938461422920227\n",
      "Training set loss--iteration: 4700  Loss: 0.30348512530326843\n",
      "Training set loss--iteration: 4800  Loss: 0.1352754384279251\n",
      "Training set loss--iteration: 4900  Loss: 0.13583579659461975\n",
      "Training set loss--iteration: 5000  Loss: 0.07089361548423767\n",
      "Training set loss--iteration: 5100  Loss: 0.28873410820961\n",
      "Training set loss--iteration: 5200  Loss: 0.07433220744132996\n",
      "Training set loss--iteration: 5300  Loss: 0.14293712377548218\n",
      "Training set loss--iteration: 5400  Loss: 0.09987736493349075\n",
      "Training set loss--iteration: 5500  Loss: 0.05756784602999687\n",
      "Training set loss--iteration: 5600  Loss: 0.9650130271911621\n",
      "Training set loss--iteration: 5700  Loss: 0.09247570484876633\n",
      "Training set loss--iteration: 5800  Loss: 0.13739308714866638\n",
      "Training set loss--iteration: 5900  Loss: 0.07226713001728058\n",
      "Training set loss--iteration: 6000  Loss: 0.16448067128658295\n",
      "Training set loss--iteration: 6100  Loss: 0.21688489615917206\n",
      "Training set loss--iteration: 6200  Loss: 0.18452291190624237\n",
      "Training set loss--iteration: 6300  Loss: 0.08265911042690277\n",
      "Training set loss--iteration: 6400  Loss: 0.12153685837984085\n",
      "Training set loss--iteration: 6500  Loss: 0.1260027438402176\n",
      "Training set loss--iteration: 6600  Loss: 0.16637076437473297\n",
      "Training set loss--iteration: 6700  Loss: 0.1415172517299652\n",
      "Training set loss--iteration: 6800  Loss: 0.40689072012901306\n",
      "Training set loss--iteration: 6900  Loss: 0.18059973418712616\n",
      "Training set loss--iteration: 7000  Loss: 0.09473904222249985\n",
      "Training set loss--iteration: 7100  Loss: 0.10484382510185242\n",
      "Training set loss--iteration: 7200  Loss: 0.12249349057674408\n",
      "Training set loss--iteration: 7300  Loss: 0.07718324661254883\n",
      "Training set loss--iteration: 7400  Loss: 0.07886764407157898\n",
      "Training set loss--iteration: 7500  Loss: 0.06394106149673462\n",
      "Training set loss--iteration: 7600  Loss: 0.08246102184057236\n",
      "Training set loss--iteration: 7700  Loss: 0.16392117738723755\n",
      "Training set loss--iteration: 7800  Loss: 0.06886409223079681\n",
      "Training set loss--iteration: 7900  Loss: 0.09243171662092209\n",
      "Training set loss--iteration: 8000  Loss: 0.2803666293621063\n",
      "Training set loss--iteration: 8100  Loss: 0.0693897008895874\n",
      "Training set loss--iteration: 8200  Loss: 0.04035426676273346\n",
      "Training set loss--iteration: 8300  Loss: 0.09947876632213593\n",
      "Training set loss--iteration: 8400  Loss: 0.1359708160161972\n",
      "Training set loss--iteration: 8500  Loss: 0.2630655765533447\n",
      "Training set loss--iteration: 8600  Loss: 0.06606804579496384\n",
      "Training set loss--iteration: 8700  Loss: 0.036804839968681335\n",
      "Training set loss--iteration: 8800  Loss: 0.10335198789834976\n",
      "Training set loss--iteration: 8900  Loss: 0.10975705832242966\n",
      "Training set loss--iteration: 9000  Loss: 3.2264974117279053\n",
      "Training set loss--iteration: 9100  Loss: 0.0801890566945076\n",
      "Training set loss--iteration: 9200  Loss: 0.10683783888816833\n",
      "Training set loss--iteration: 9300  Loss: 0.1250954568386078\n",
      "Training set loss--iteration: 9400  Loss: 2.218923330307007\n",
      "Training set loss--iteration: 9500  Loss: 0.09948010742664337\n",
      "Training set loss--iteration: 9600  Loss: 0.04789697006344795\n",
      "Training set loss--iteration: 9700  Loss: 0.8883622884750366\n",
      "Training set loss--iteration: 9800  Loss: 0.09553170204162598\n",
      "Training set loss--iteration: 9900  Loss: 0.06289361417293549\n",
      "Training set loss--iteration: 10000  Loss: 0.3554726243019104\n",
      "Training set loss--iteration: 10100  Loss: 0.25915059447288513\n",
      "Training set loss--iteration: 10200  Loss: 0.04629518464207649\n",
      "Training set loss--iteration: 10300  Loss: 0.20953354239463806\n",
      "Training set loss--iteration: 10400  Loss: 0.0345725491642952\n",
      "Training set loss--iteration: 10500  Loss: 0.6586864590644836\n",
      "Training set loss--iteration: 10600  Loss: 0.12082591652870178\n",
      "Training set loss--iteration: 10700  Loss: 0.0787445604801178\n",
      "Training set loss--iteration: 10800  Loss: 0.06648708879947662\n",
      "Training set loss--iteration: 10900  Loss: 7.895450115203857\n",
      "Training set loss--iteration: 11000  Loss: 2.6290338039398193\n",
      "Training set loss--iteration: 11100  Loss: 0.05053762346506119\n",
      "Training set loss--iteration: 11200  Loss: 0.0535154715180397\n",
      "Training set loss--iteration: 11300  Loss: 0.3107820451259613\n",
      "Training set loss--iteration: 11400  Loss: 0.812075674533844\n",
      "Training set loss--iteration: 11500  Loss: 0.04672202095389366\n",
      "Training set loss--iteration: 11600  Loss: 0.06645365059375763\n",
      "Training set loss--iteration: 11700  Loss: 0.07176067680120468\n",
      "Training set loss--iteration: 11800  Loss: 0.1580316722393036\n",
      "Training set loss--iteration: 11900  Loss: 0.47457268834114075\n",
      "Training set loss--iteration: 12000  Loss: 0.360757440328598\n",
      "Training set loss--iteration: 12100  Loss: 0.15367966890335083\n",
      "Training set loss--iteration: 12200  Loss: 0.03290031850337982\n",
      "Training set loss--iteration: 12300  Loss: 0.06085335835814476\n",
      "Training set loss--iteration: 12400  Loss: 0.06749212741851807\n",
      "Training set loss--iteration: 12500  Loss: 0.03728247433900833\n",
      "Training set loss--iteration: 12600  Loss: 0.06523934006690979\n",
      "Training set loss--iteration: 12700  Loss: 0.850644052028656\n",
      "Training set loss--iteration: 12800  Loss: 0.12296231836080551\n",
      "Training set loss--iteration: 12900  Loss: 1.8413535356521606\n",
      "Training set loss--iteration: 13000  Loss: 0.07937732338905334\n",
      "Training set loss--iteration: 13100  Loss: 4.219506740570068\n",
      "Training set loss--iteration: 13200  Loss: 0.12181329727172852\n",
      "Training set loss--iteration: 13300  Loss: 0.05963841453194618\n",
      "Training set loss--iteration: 13400  Loss: 0.11336386948823929\n",
      "Training set loss--iteration: 13500  Loss: 0.043341170996427536\n",
      "Training set loss--iteration: 13600  Loss: 0.24918563663959503\n",
      "Training set loss--iteration: 13700  Loss: 0.10007578134536743\n",
      "Training set loss--iteration: 13800  Loss: 0.06677022576332092\n",
      "Training set loss--iteration: 13900  Loss: 0.1882922649383545\n",
      "Training set loss--iteration: 14000  Loss: 0.07094333320856094\n",
      "Training set loss--iteration: 14100  Loss: 2.189955711364746\n",
      "Training set loss--iteration: 14200  Loss: 0.13223470747470856\n",
      "Training set loss--iteration: 14300  Loss: 0.6322236657142639\n",
      "Training set loss--iteration: 14400  Loss: 2.3065919876098633\n",
      "Training set loss--iteration: 14500  Loss: 0.4581559896469116\n",
      "Training set loss--iteration: 14600  Loss: 0.04187396913766861\n",
      "Training set loss--iteration: 14700  Loss: 0.09143557399511337\n",
      "Training set loss--iteration: 14800  Loss: 0.07684150338172913\n",
      "Training set loss--iteration: 14900  Loss: 0.29729795455932617\n",
      "Training set loss--iteration: 15000  Loss: 0.06723925471305847\n",
      "Training set loss--iteration: 15100  Loss: 0.03850514441728592\n",
      "Training set loss--iteration: 15200  Loss: 0.08289864659309387\n",
      "Training set loss--iteration: 15300  Loss: 0.09202009439468384\n",
      "Training set loss--iteration: 15400  Loss: 0.042500294744968414\n",
      "Training set loss--iteration: 15500  Loss: 0.08546825498342514\n",
      "Training set loss--iteration: 15600  Loss: 0.16261716187000275\n",
      "Training set loss--iteration: 15700  Loss: 0.5870885252952576\n",
      "Training set loss--iteration: 15800  Loss: 0.12535563111305237\n",
      "Training set loss--iteration: 15900  Loss: 0.29509636759757996\n",
      "Training set loss--iteration: 16000  Loss: 0.6510112881660461\n",
      "Training set loss--iteration: 16100  Loss: 0.07415913045406342\n",
      "Training set loss--iteration: 16200  Loss: 0.2932583689689636\n",
      "Training set loss--iteration: 16300  Loss: 0.04687545448541641\n",
      "Training set loss--iteration: 16400  Loss: 0.0673801526427269\n",
      "Training set loss--iteration: 16500  Loss: 0.16749197244644165\n",
      "Training set loss--iteration: 16600  Loss: 0.07699967920780182\n",
      "Training set loss--iteration: 16700  Loss: 0.07277201861143112\n",
      "Training set loss--iteration: 16800  Loss: 0.11098144948482513\n",
      "Training set loss--iteration: 16900  Loss: 0.19500231742858887\n",
      "Training set loss--iteration: 17000  Loss: 0.390125572681427\n",
      "Training set loss--iteration: 17100  Loss: 0.06785978376865387\n",
      "Training set loss--iteration: 17200  Loss: 0.7052301168441772\n",
      "Training set loss--iteration: 17300  Loss: 1.9922728538513184\n",
      "Training set loss--iteration: 17400  Loss: 0.05436257645487785\n",
      "Training set loss--iteration: 17500  Loss: 1.2517929077148438\n",
      "Training set loss--iteration: 17600  Loss: 0.05256393924355507\n",
      "Training set loss--iteration: 17700  Loss: 1.2311686277389526\n",
      "Training set loss--iteration: 17800  Loss: 0.04796929284930229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 15000\n",
    "#create model & training set\n",
    "input_dim = 64*4\n",
    "num_epochs = int(n_iters / (len(featuresTrainRandomSplit) / batch_size))\n",
    "\n",
    "\n",
    "\n",
    "model_dropout = ANNModel(input_dim, hidden_dim =10, output_dim = 1, dropout = True)\n",
    "#run training\n",
    "run_model_training(model_dropout, train_set,test = test_set, batch_size=batch_size, n_iters=n_iters, num_epochs=num_epochs, learning_rate = .002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Pearson R, p-value) of regression for antibodies in training set: (0.9997460234792998, 0.0)\n",
      "(Pearson R, p-value) of regression for antibodies in testing set: (0.15869522241172088, 0.02480253165583189)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "pred_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "train = torch.utils.data.TensorDataset(featuresTrainRandomSplit,targetsTrainRandomSplit)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in train_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"train\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "test = torch.utils.data.TensorDataset(featuresTestRandomSplit,targetsTestRandomSplit)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in test_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"test\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "    \n",
    "# Generate predictions for the entire test and training sets\n",
    "# These predictions are sorted into a Pandas DataFrame which is a handy means of storing data like this\n",
    "# and compatible with the seaborn plotting library below.\n",
    "# \n",
    "#predictions = get_all_predictions(model, targetsTrainRandomSplit, featuresTrainRandomSplit, targetsTestRandomSplit, featuresTestRandomSplit)\n",
    "train_predictions = pred_all.loc[lambda x:x.kind ==\"train\"]\n",
    "test_predictions = pred_all.loc[lambda x:x.kind ==\"test\"]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "(Pearson R, p-value) of regression for antibodies in training set: {stats.pearsonr(train_predictions.labels, train_predictions.predictions)}\n",
    "(Pearson R, p-value) of regression for antibodies in testing set: {stats.pearsonr(test_predictions.labels, test_predictions.predictions)}\n",
    "\"\"\")\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Pearson R, p-value) of regression for antibodies in training set: (0.9576438366752329, 1.1830969441110376e-182)\n",
      "(Pearson R, p-value) of regression for antibodies in testing set: (0.16444543788491311, 0.019970781568790192)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "pred_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "train = torch.utils.data.TensorDataset(featuresTrainRandomSplit,targetsTrainRandomSplit)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in train_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model_dropout(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"train\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "test = torch.utils.data.TensorDataset(featuresTestRandomSplit,targetsTestRandomSplit)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in test_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model_dropout(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"test\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "    \n",
    "# Generate predictions for the entire test and training sets\n",
    "# These predictions are sorted into a Pandas DataFrame which is a handy means of storing data like this\n",
    "# and compatible with the seaborn plotting library below.\n",
    "# \n",
    "#predictions = get_all_predictions(model, targetsTrainRandomSplit, featuresTrainRandomSplit, targetsTestRandomSplit, featuresTestRandomSplit)\n",
    "train_predictions = pred_all.loc[lambda x:x.kind ==\"train\"]\n",
    "test_predictions = pred_all.loc[lambda x:x.kind ==\"test\"]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "(Pearson R, p-value) of regression for antibodies in training set: {stats.pearsonr(train_predictions.labels, train_predictions.predictions)}\n",
    "(Pearson R, p-value) of regression for antibodies in testing set: {stats.pearsonr(test_predictions.labels, test_predictions.predictions)}\n",
    "\"\"\")\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Pearson R, p-value) of regression for antibodies in training set: (0.9338394086995254, 3.4800389065790096e-151)\n",
      "(Pearson R, p-value) of regression for antibodies in testing set: (0.25246544880339183, 0.0003102081946196595)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "pred_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "train = torch.utils.data.TensorDataset(featuresTrainRandomSplit,targetsTrainRandomSplit)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in train_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model_dropout2(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"train\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "test = torch.utils.data.TensorDataset(featuresTestRandomSplit,targetsTestRandomSplit)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "for images, labels in test_loader:\n",
    "    data = Variable(images.view(-1,64*4)).float()\n",
    "    # Forward propagation\n",
    "    outputs = model_dropout2(data)\n",
    "    pred_labels = pd.concat([\n",
    "                            pd.Series(float(e) for e in outputs.data).rename(\"predictions\"),\n",
    "                            pd.Series(float(e) for e in labels).rename(\"labels\"),\n",
    "                            pd.Series([e for e in np.array(data.data)]).rename(\"features\"),\n",
    "    ],axis=1).assign(kind=\"test\")\n",
    "    pred_all= pd.concat([pred_all, pred_labels])\n",
    "    \n",
    "    \n",
    "# Generate predictions for the entire test and training sets\n",
    "# These predictions are sorted into a Pandas DataFrame which is a handy means of storing data like this\n",
    "# and compatible with the seaborn plotting library below.\n",
    "# \n",
    "#predictions = get_all_predictions(model, targetsTrainRandomSplit, featuresTrainRandomSplit, targetsTestRandomSplit, featuresTestRandomSplit)\n",
    "train_predictions = pred_all.loc[lambda x:x.kind ==\"train\"]\n",
    "test_predictions = pred_all.loc[lambda x:x.kind ==\"test\"]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "(Pearson R, p-value) of regression for antibodies in training set: {stats.pearsonr(train_predictions.labels, train_predictions.predictions)}\n",
    "(Pearson R, p-value) of regression for antibodies in testing set: {stats.pearsonr(test_predictions.labels, test_predictions.predictions)}\n",
    "\"\"\")\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data = train_predictions\n",
    "sns.scatterplot(x = \"labels\", y = \"predictions\", data = data)\n",
    "\n",
    "data = test_predictions\n",
    "sns.scatterplot(x = \"labels\", y = \"predictions\", data = data)\n",
    "\n",
    "plt.gca().set_title(f\"\"\"bag of k-mers training and test performance for motif subregions\n",
    "(Pearson R, p-value) of regression for antibodies in training set: {stats.pearsonr(train_predictions.labels, train_predictions.predictions)}\n",
    "(Pearson R, p-value) of regression for antibodies in testing set: {stats.pearsonr(test_predictions.labels, test_predictions.predictions)}\"\"\")\n",
    "\n",
    "plt.gcf().set_size_inches(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
